{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399be64-44d3-45e4-9b43-3757a9b92daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray import serve\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f391603-346b-4f08-a5b9-dee050b77409",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Outline\n",
    "\n",
    "-   Deployments\n",
    "    -   Resources (CPU/GPU/custom)\n",
    "    -   Runtime environments support, usage (functionality)\n",
    "    -   Bound deployments, ServeHandles\n",
    "-   Scaling and Performance\n",
    "    -   Replicas\n",
    "        -   num_replicas, autoscaling_config, max_concurrent_queries\n",
    "    -   Request batching\n",
    "-   Composition Patterns\n",
    "    -   Imperative\n",
    "    -   Declarative / Graph Deployment API\n",
    "-   Architecture / Under-the-hood\n",
    "    -   Ray cluster perspective - processes / workers / actors\n",
    "    -   Request routing, queuing, load balancing in Serve\n",
    "\n",
    "### Example scenario: computer vision services\n",
    "\n",
    "For our example use case, we’ll see how to leverage Ray Serve to host a CV segmentation\n",
    "model and how to enhance it using additional services such as image preprocessing.\n",
    "\n",
    "### Context: Ray AIR\n",
    "\n",
    "Ray AIR is the Ray AI Runtime, a set of high-level easy-to-use APIs for\n",
    "ingesting data, training models – including reinforcement learning\n",
    "models – tuning those models and then serving them.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=600 loading=\"lazy\"/>\n",
    "\n",
    "Key principles behind Ray and Ray AIR are\n",
    "* Performance\n",
    "* Developer experience and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc41d8f-4332-4359-b957-f26bc79de7a3",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "\n",
    "Serve is a microservices framework for serving ML – the model serving\n",
    "component of Ray AIR.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "# Deployments\n",
    "\n",
    "`Deployment` is the fundamental user-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "## Our First Service\n",
    "\n",
    "Let’s jump right in and get something simple up and running on Ray\n",
    "Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48953d75-e69a-4fe3-adf8-079edc953683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class TextConverter:\n",
    "    def convert(self, text):\n",
    "        return \"***\" + str.upper(text) + \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62641bcc-f2a0-48ee-ac1d-182688552af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_handle = serve.run(TextConverter.bind())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7ce21-ee2a-48cb-8043-e986a4522e89",
   "metadata": {},
   "source": [
    "## Key APIs and concepts\n",
    "\n",
    "`Deployment` represents a service and is created with the `@serve.deployment` decorator\n",
    "* As end users, we don't instantiate `Deployment`s directly\n",
    "* Ray will create them as actors, per our scaling requirements\n",
    "\n",
    "A __bound deployment__ is created with the `.bind` class method on the deployment class\n",
    "* e.g., `TextConverter.bind(msg=\"Yes...\")` above creates a bound deployment\n",
    "* `.bind` allows us to provide constructor params for the deployment class (the `msg` param above)\n",
    "* bound deployments *can* be passed to other deployments via `.bind` -- this is one way to compose services\n",
    "* We can pass a bound deployment to `serve.run(...)`\n",
    "    * to start a service\n",
    "    * to obtain a `ServeHandle`\n",
    "\n",
    "A `ServeHandle` can be used to invoke services through the Python API\n",
    "* At runtime, services can call other services via serve handles\n",
    "    * Bound deployments provided to deployment constructors via `.bind` become serve handles at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50986b5-b5d5-48e1-b7ce-d5fa27a2121e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(app_handle))\n",
    "print(app_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f5c3e-f185-4d7d-9f97-48a0ffee66dc",
   "metadata": {},
   "source": [
    "Look at Actors in the dashboard. Why are deployment replicas actors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085cc7d1-dad5-4c82-8073-2b1a44ecd2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_handle.convert.remote(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3303f-1a92-4e19-abd0-1b255aa7704c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(app_handle.convert.remote(\"cat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e0c5b-c19f-4c15-9792-42cae983d7e0",
   "metadata": {},
   "source": [
    "Ok, we have a minimal deployment built and running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167f6ee-55c1-469f-b945-8c37fce4c581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2d2f7-1076-4aff-aa52-7b08d36a323d",
   "metadata": {},
   "source": [
    "What do we want to do next?\n",
    "* Support some image processing\n",
    "* Support HTTP\n",
    "\n",
    "Then...\n",
    "* Image segmentation with SegFormer\n",
    "* Service composition -- e.g., grayscale/resize/sharpen/etc. and then segment\n",
    "\n",
    "And finally...\n",
    "* Manage GPUs (and resources generally)\n",
    "* Multiple replicas, autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15073d2c-f859-4454-a4ef-32b00943add1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Threshold:\n",
    "    def __init__(self, threshold: int):\n",
    "        self._threshold = threshold # initial state\n",
    "    \n",
    "    def get_response(self, image):\n",
    "        new_image = np.zeros_like(image)\n",
    "        new_image[image > self._threshold] = 255\n",
    "        new_image[new_image < 255] = 0\n",
    "        return new_image\n",
    "\n",
    "app_handle = serve.run(Threshold.bind(threshold=128), name='hello_image_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f06f9-08ea-484f-b832-3eb99bd76d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im = Image.open(\"images/cat.jpg\")\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df12907-1999-44df-b111-6f7f45d99681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im = im.resize((512,384))\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ca27c-4179-45bf-87c0-d031aff283c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(im.getdata()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a59531-8255-4432-a540-f7e29b23c1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = np.array(im.getdata())\n",
    "arr = arr.reshape(-1, 512, 3)\n",
    "\n",
    "plt.imshow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b923d3-c2f5-41d9-bd5b-2e51be1b6d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(arr.mean(axis=2), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a0b5e-1c26-4ce2-bff2-0d91afbf1f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ref = app_handle.get_response.remote(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6550f5-d977-4438-9f20-a68663f5ad3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ray.get(output_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701bd2e-1042-4d27-af9a-16f2b4261550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('hello_image_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40c36f-8d6c-4bed-83cb-759bd9f8c0e5",
   "metadata": {},
   "source": [
    "Add HTTP ... this is a bit messier just because of conversion between bytes, arrays, and HTTP tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa8d0-4d4a-41bd-b206-9de536da5c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Threshold:\n",
    "    def __init__(self, threshold: int):\n",
    "        self._threshold = threshold # initial state\n",
    "\n",
    "    def get_response(self, image):\n",
    "        new_image = np.zeros_like(image)\n",
    "        new_image[image > self._threshold] = 255\n",
    "        new_image[new_image < 255] = 0\n",
    "        return new_image\n",
    "    \n",
    "    # a lot of boilerplate as HTTP adapter for images + ndarrays (a text/JSON example would be about 3 lines)\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        import numpy as np\n",
    "        import io\n",
    "        from imageio import v3 as iio\n",
    "        from fastapi import Response\n",
    "\n",
    "        # async collect POST body\n",
    "        body = await request.body()\n",
    "        \n",
    "        # unpickle serialized data\n",
    "        image = pickle.loads(body)\n",
    "        \n",
    "        # get NDArray for our image processing\n",
    "        data = np.array(image)\n",
    "        \n",
    "        # invoke existing business logic\n",
    "        transformed_data = self.get_response(data)\n",
    "        \n",
    "        # convert to image\n",
    "        transformed_image = Image.fromarray(transformed_data.astype(np.uint8))\n",
    "        \n",
    "        # prepare output buffer\n",
    "        with io.BytesIO() as buf:\n",
    "            iio.imwrite(buf, transformed_image, plugin=\"pillow\", format=\"JPEG\")\n",
    "            im_bytes = buf.getvalue()\n",
    "        \n",
    "        # prepare and return HTTP Response\n",
    "        headers = {'Content-Disposition': 'inline'}\n",
    "        return Response(im_bytes, headers=headers, media_type='image/jpeg')\n",
    "\n",
    "app_handle = serve.run(Threshold.bind(threshold=128), name='hello_image_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd72e73-02f1-4a51-af72-ee7012b785eb",
   "metadata": {},
   "source": [
    "Threshold our cat via HTTP\n",
    "\n",
    "(if we are working with arrays on the client side and want to make an image from an array, we'd call `Image.fromarray(my_array)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b088b7-56b2-44be-bad1-ff780b835179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:8000/\", data = pickle.dumps(im)) # uncompressed\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582f1de-a173-4a88-8a18-e20d28c710be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001982f-e6cf-4eae-92d4-ae23086e6505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('hello_image_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8e3c2-a65e-4011-8da1-af48234735a5",
   "metadata": {},
   "source": [
    "## Build a semantic segmentation service on SegFormer\n",
    "\n",
    "At this point, we've done all the hard work -- we know the structure of our service code.\n",
    "\n",
    "In this use case, we're going to build and test\n",
    "* SegFormer-based segmentation service\n",
    "* Image prep service (as a demo, we'll just convert the image to grayscale, but feel free to experiment with other transformations)\n",
    "* an Ingress service, to separate the HTTP handling code from our other components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15aad6-c1dd-47ca-9960-021cc0aa4b08",
   "metadata": {},
   "source": [
    "### Segmentation service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbfd65-422d-4797-924a-406047d371b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Segmenter:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "        self.feature_extractor = SegformerFeatureExtractor.from_pretrained(model_name, do_reduce_labels=True)\n",
    "\n",
    "    def segment(self, image) -> list[np.ndarray]: # can process PIL Image, or torch/np tensor\n",
    "\n",
    "        batch = [image]\n",
    "        # Set the device on which PyTorch will run.\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)  # Move the model to specified device.\n",
    "        self.model.eval()  # Set the model in evaluation mode on test data.\n",
    "\n",
    "        # The feature extractor processes raw images.\n",
    "        inputs = self.feature_extractor(images=batch, return_tensors=\"pt\")\n",
    "\n",
    "        # The model is applied to input images in the inference step.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        # Post-process the output for display.\n",
    "        image_sizes = [image.size[::-1] for image in batch]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997f2f6-e19f-4b83-a0a7-7a74eee4bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter.bind(\"nvidia/segformer-b0-finetuned-ade-512-512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430d38e-60fb-4a6d-bad1-43e36ca05a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_handle = serve.run(segmenter, name='seg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4a4cf-442c-4c04-9650-0a9a37e0d0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = app_handle.segment.remote(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b3f32-4582-49aa-8f8c-051df293d9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ray.get(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b774bf-cb2f-42e0-8b86-9220f0f6e4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('seg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d0aa7-4b55-41af-bb29-d53b7a978c25",
   "metadata": {},
   "source": [
    "Ok, out segmenter service works!\n",
    "\n",
    "#### Next, we'll create a simple image processor service which does some preprocessing -- ours will \"sharpen\" -- and then segmenting.\n",
    "\n",
    "The idea is that a user can call our image processor to do the combined tasks, and it will let us demonstrate how to __compose__ two Ray Serve services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea4bff-a85a-4ee3-8126-4362a61b5cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class SharpenThenSegment:\n",
    "    def __init__(self, segmenter_service):\n",
    "        self._segmenter_service = segmenter_service\n",
    "\n",
    "    async def process(self, image):\n",
    "        # first we do some custom preprocessing -- in this case a sharpening\n",
    "        enhancer = ImageEnhance.Sharpness(image)\n",
    "        processed_image = enhancer.enhance(8)\n",
    "        # now we use the segmenter -- note the *remote* and *await*\n",
    "        result = await self._segmenter_service.segment.remote(processed_image)\n",
    "        return await result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e3870-6131-4779-a3aa-251b16176343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter.bind(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "sharpen_then_segment = SharpenThenSegment.bind(segmenter)\n",
    "\n",
    "app_handle = serve.run(sharpen_then_segment, name='composition_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fcfbf-086b-442c-a23c-03d6dfb4b570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = app_handle.process.remote(im)\n",
    "\n",
    "plt.imshow(ray.get(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fded6e-cce5-4f96-89f3-beecae7b1770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('composition_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3dc64-5c0a-4567-8ff1-57b6037abde8",
   "metadata": {},
   "source": [
    "Now we'll wrap all of this behind an ingress service, to demontrate more composition and factor out the HTTP handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20742468-55cb-4f29-b8f9-4eca80a8b354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Ingress:\n",
    "    def __init__(self, processor_service):\n",
    "        self._processor_service = processor_service\n",
    "    \n",
    "    # a lot of boilerplate as HTTP adapter for images + ndarrays (a text/JSON example would be about 3 lines)\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        import numpy as np\n",
    "        import io\n",
    "        from imageio import v3 as iio\n",
    "        from fastapi import Response\n",
    "\n",
    "        # async collect POST body\n",
    "        body = await request.body()\n",
    "        \n",
    "        # unpickle serialized data\n",
    "        image = pickle.loads(body)\n",
    "        \n",
    "        # invoke existing business logic; await to get obj ref to result\n",
    "        ref = await self._processor_service.process.remote(image)\n",
    "        \n",
    "        # await the actual data, since we need it for the remaining conversion steps\n",
    "        transformed_data = await ref\n",
    "        \n",
    "        # convert to image\n",
    "        transformed_image = Image.fromarray(transformed_data.astype(np.uint8))\n",
    "        \n",
    "        # prepare output buffer\n",
    "        with io.BytesIO() as buf:\n",
    "            iio.imwrite(buf, transformed_image, plugin=\"pillow\", format=\"JPEG\")\n",
    "            im_bytes = buf.getvalue()\n",
    "        \n",
    "        # prepare and return HTTP Response\n",
    "        headers = {'Content-Disposition': 'inline'}\n",
    "        return Response(im_bytes, headers=headers, media_type='image/jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9c33d-5a32-450a-b24f-bd3c01457abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter.bind(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "sharpen_then_segment = SharpenThenSegment.bind(segmenter)\n",
    "ingress = Ingress.bind(sharpen_then_segment)\n",
    "\n",
    "app_handle = serve.run(ingress, name='composition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbc2ff-d893-40e8-a25c-11fc1f75af81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:8000/\", data = pickle.dumps(im))\n",
    "\n",
    "Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dbd8b-52b6-4f06-a94d-f785fd27179e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('composition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a4917-c20b-4da2-b27c-0517b99e27ce",
   "metadata": {},
   "source": [
    "## Specifying service resources and scaling\n",
    "\n",
    "### Resources\n",
    "\n",
    "Resources -- typically GPUs, althoufgh  can be specified on a per-deployment basis and, if we want, in fractional units, via the `ray_actor_options` parameter on the `@serve.deployment` decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4aeb33-3b28-4072-ad48-d2f1b10c29d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Resources can include\n",
    "* `num_cpus`\n",
    "* `num_gpus`\n",
    "* `resources` dictionary containing custom resources\n",
    "    * custom resources are tracked and accounted as symbols (or tags) in order to match actors to workers\n",
    "    \n",
    "Example\n",
    "```python\n",
    "@serve.deployment(ray_actor_options={'num_cpus' : 2, 'num_gpus' : 2, resources : {\"my_super_accelerator\": 1}})\n",
    "class Demo:\n",
    "    ...\n",
    "```\n",
    "\n",
    "The purpose of the declarative resource mechanism is to allow Ray to place code on suitable nodes in a heterogeneous cluster without our having know which nodes have which resources to where our code should run.\n",
    "\n",
    "> Best practice: if some nodes have a distinguising feature, mark and request it as a resource, rather than trying to determine which nodes are present and where your code will run.\n",
    "\n",
    "For more details, see https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#resource-management-cpus-gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0551519-07d2-4993-8fc8-397842ae2a88",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Replicas and autoscaling\n",
    "\n",
    "Each deployment can have its own resource management and autoscaling configuration, with several options for scaling.\n",
    "\n",
    "By default -- if nothing specified, as in our examples above -- the default is a single. We can specify a larger, constant number of replicas in the decorator:\n",
    "```python\n",
    "@serve.deployment(num_replicas=3)\n",
    "```\n",
    "\n",
    "For autoscaling, instead of `num_replicas`, we provide an `autoscaling_config` dictionary. With autoscaling, we can specify a minimum and maximum range for the number of replicas, the initial replica count, a load target, and more.\n",
    "\n",
    "Here is example of extended configuration -- see https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation for more details:\n",
    "\n",
    "```python\n",
    "@serve.deployment(\n",
    "    autoscaling_config={\n",
    "        'min_replicas': 1,\n",
    "        'initial_replicas': 2,\n",
    "        'max_replicas': 5,\n",
    "        'target_num_ongoing_requests_per_replica': 10,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "`min_replicas` can also be set to zero to create a \"serverless\" style design: in exchange for potentially slower startup, no actors (or their CPU/GPU resources) need to be permanently reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f88fe-73db-41b7-a0d0-df0e454c21d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.3}, autoscaling_config={ 'min_replicas': 2, 'max_replicas': 3 })\n",
    "class Segmenter:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "        self.feature_extractor = SegformerFeatureExtractor.from_pretrained(model_name, do_reduce_labels=True)\n",
    "\n",
    "    def segment(self, image) -> list[np.ndarray]: # can process PIL Image, or torch/np tensor\n",
    "\n",
    "        batch = [image]\n",
    "        device = 'cuda:0' # explicitly name GPU for demo\n",
    "        self.model.to(device)\n",
    "        self.model.eval()  # Set the model in evaluation mode on test data.\n",
    "\n",
    "        inputs = self.feature_extractor(images=batch, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        image_sizes = [image.size[::-1] for image in batch]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aeb70d-fd27-4944-958e-dac86b2e1f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter.bind(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "sharpen_then_segment = SharpenThenSegment.bind(segmenter)\n",
    "ingress = Ingress.bind(sharpen_then_segment)\n",
    "\n",
    "app_handle = serve.run(ingress, name='resource_and_scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ec19c-0fe4-41e3-8983-de7c14ab88ff",
   "metadata": {},
   "source": [
    "Notice the two replicas in the Serve and Actors dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a7911-cfd2-4587-9f74-2d7faae084c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.open(BytesIO(requests.post(\"http://localhost:8000/\", data = pickle.dumps(im)).content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75dcd20-334e-4e94-97f0-701bacbf3f4f",
   "metadata": {},
   "source": [
    "Observe that GPU memory is being consumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575eab7-b455-456e-8601-c95d1fdca60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('resource_and_scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347781f-13ce-468f-a3f7-06aa4262d4a6",
   "metadata": {
    "id": "778ede38-d221-4f45-8e2d-71ec0702aae4",
    "tags": []
   },
   "source": [
    "## Alternative composition pattern: Deployment Graph API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48a6a1-4f60-4ca7-889a-f31866cbe4c7",
   "metadata": {
    "id": "6eae86f3-34a4-4f59-87ba-df8d9a0d3928"
   },
   "source": [
    "What is the Deployment Graph API?\n",
    "\n",
    "* The Deployment Graph API lets us separate the flow of calls from the logic inside our services.\n",
    "\n",
    "Why might we want to use the Deployment Graph (DAG) API to separate flow from logic?\n",
    "\n",
    "* It may be valuable to add a layer of indirection – or abstraction – so that we can more easily create and compose reusable services\n",
    "* The DAG API lets us use similar patterns across the Ray platform (e.g., Ray Workflow)\n",
    "    * We can learn one general pattern for graphs and use that intuition in multiple places in our Ray applications\n",
    "* Although we compose one DAG, we retain the key Ray Serve features of granular autoscaling and resource allocation\n",
    "\n",
    "Let’s reproduce our chat service flow using the Deployment Graph API\n",
    "\n",
    "#### Getting started with deployment graphs\n",
    "\n",
    "For this example, we'll have a linear graph (flow)\n",
    "\n",
    "<span style='color:red;font-size:18pt;'>REPLACE THIS IMAGE</span>\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_simple.png' width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9def2-6de2-43a8-a465-aa3ea2478fba",
   "metadata": {
    "id": "3500503d-0831-47ae-88e0-88b8a8a7e2cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.serve.dag import InputNode\n",
    "from ray.serve.drivers import DAGDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9625b-4e2b-4d72-b314-0580096fb311",
   "metadata": {},
   "source": [
    "`InputNode` is a special type of graph node, defined by Ray Serve, which represents values supplied to our service endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f22495-689c-403a-b977-5c5056922fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "async def unpack_request(request: Request):\n",
    "    body = await request.body()\n",
    "    image = pickle.loads(body)\n",
    "    return image\n",
    "\n",
    "@serve.deployment\n",
    "def sharpener(image):\n",
    "    enhancer = ImageEnhance.Sharpness(image)\n",
    "    sharpened_image = enhancer.enhance(8)\n",
    "    return sharpened_image\n",
    "\n",
    "# in sequence, we'll segment next -- but we can re-use the segmenter we've already defined\n",
    "\n",
    "@serve.deployment\n",
    "def pack_response(image):\n",
    "    import io\n",
    "    from imageio import v3 as iio\n",
    "    from fastapi import Response\n",
    "\n",
    "    transformed_data = image # await???\n",
    "    transformed_image = Image.fromarray(transformed_data.astype(np.uint8))\n",
    "    with io.BytesIO() as buf:\n",
    "        iio.imwrite(buf, transformed_image, plugin=\"pillow\", format=\"JPEG\")\n",
    "        im_bytes = buf.getvalue()\n",
    "        \n",
    "    headers = {'Content-Disposition': 'inline'}\n",
    "    return Response(im_bytes, headers=headers, media_type='image/jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b3685-de87-44e2-9311-fb6ed8eda355",
   "metadata": {
    "id": "d2463417-b8c8-41ee-a259-7708d3952ff8"
   },
   "source": [
    "Here is a minimal, linear pipeline performs the processing we implemented earlier.\n",
    "\n",
    "We build up the graph step by step, `bind`ing each deployment to its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aec5db-d963-42e0-9978-70e3eec93ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter.bind(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "with InputNode() as http_request:\n",
    "    input_image = unpack_request.bind(http_request)\n",
    "    sharpened_image = sharpener.bind(input_image)\n",
    "    segmented = segmenter.segment.bind(sharpened_image)    \n",
    "    response = pack_response.bind(segmented)\n",
    "    \n",
    "graph = DAGDriver.bind(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e5377-0763-494d-80c3-e80cb7df7dc3",
   "metadata": {},
   "source": [
    "We start the application by calling `serve.run()` on the DAGDriver, a Ray Serve component which routes HTTP requests through your call graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f855710-5c45-43e0-8446-97d231cd5ac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6552ef01-a62b-497f-b578-8fc20edc6000",
    "outputId": "ad1eb72e-6c8c-473f-a92f-89d5bbebdeb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_handle = serve.run(graph, name='basic_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ef32a-702e-42b5-97e7-ed291fc755a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.open(BytesIO(requests.post(\"http://localhost:8000/\", data = pickle.dumps(im)).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df0103-07f6-45c3-a643-472930f26d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb75ac-aa59-4d69-9740-a44e61d13e08",
   "metadata": {},
   "source": [
    "## Architecture / under-the-hood\n",
    "\n",
    "### Ray cluster perspective: actors\n",
    "\n",
    "In Ray, user code is executed by worker processes. These workers can run tasks (stateless functions) or actors (stateful class instances).\n",
    "\n",
    "Ray Serve is built on actors, allowing deployments to collect expensive state once (such as loading a ML model) and to reuse it across many service requests.\n",
    "\n",
    "Although you may never need to code any Ray tasks or actors yourself, your Ray Serve application has full access to those cluster capabilities and you may wish to use them to implement other functionality (e.g., service or operations that don't need to accept HTTP traffic). More information is at https://docs.ray.io/en/latest/ray-core/walkthrough.html\n",
    "\n",
    "### Serve design\n",
    "\n",
    "Under the hood, a few other actors are used to make up a serve instance.\n",
    "\n",
    "* Controller: A global actor unique to each Serve instance is responsible for managing other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "* HTTP Proxy: By default there is one HTTP proxy actor on the head node that accepts incoming requests, forwards them to replicas, and responds once they are completed. For scalability and high availability, you can also run a proxy on each node in the cluster via the location field of http_options.\n",
    "\n",
    "* Deployment Replicas: Actors that execute the code in response to a request. Each replica processes requests from the HTTP proxy.\n",
    "<img src='https://docs.ray.io/en/latest/_images/architecture-2.0.svg' width=700 />\n",
    "\n",
    "Incoming requests, once resolved to a particular deployment, are queued. The requests from the queue are assigned round-robin to available replicas as long as capacity is available. This design provides load balancing and elasticity. \n",
    "\n",
    "Capacity can be managed with the `max_concurrent_queries` parameter to the deployment decorator. This value defaults to 100 and represents the maximum number of queries that will be sent to a replica of this deployment without receiving a response. Each replica has its own queue to collect and smooth incoming request traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101cbf7-8ac2-46fa-8684-682666f566cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
