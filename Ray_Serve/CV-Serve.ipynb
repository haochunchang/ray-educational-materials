{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe29592-1d8b-43b5-a340-a9426e77588a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b62aa-fc4d-4d62-8c85-fd31d1209ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install Pillow # Requirement already satisfied: Pillow in /home/ray/anaconda3/lib/python3.9/site-packages (9.5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399be64-44d3-45e4-9b43-3757a9b92daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray import tune\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f391603-346b-4f08-a5b9-dee050b77409",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Outline\n",
    "\n",
    "-   Deployments\n",
    "    -   Resources (CPU/GPU/custom)\n",
    "    -   Runtime environments support, usage (functionality)\n",
    "    -   Bound deployments, ServeHandles\n",
    "-   Composition Patterns\n",
    "    -   Imperative\n",
    "    -   Declarative / Graph Deployment API\n",
    "-   Architecture / Under-the-hood\n",
    "    -   Ray cluster perspective - processes / workers / actors\n",
    "    -   Request routing, queuing, load balancing in Serve\n",
    "-   Scaling and Performance\n",
    "    -   Replicas\n",
    "        -   num_replicas, autoscaling_config, max_concurrent_queries\n",
    "    -   Request batching\n",
    "\n",
    "### Example scenario: computer vision services\n",
    "\n",
    "For our example use case, we’ll see how to leverage Ray Serve to host a CV segmentation\n",
    "model and how to enhance it using additional services such as image preprocessing.\n",
    "\n",
    "### Context: Ray AIR\n",
    "\n",
    "Ray AIR is the Ray AI Runtime, a set of high-level easy-to-use APIs for\n",
    "ingesting data, training models – including reinforcement learning\n",
    "models – tuning those models and then serving them.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=600 loading=\"lazy\"/>\n",
    "\n",
    "Key principles behind Ray and Ray AIR are\n",
    "* Performance\n",
    "* Developer experience and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf588b5-4b9a-4c2e-9ac0-f7b23213579a",
   "metadata": {},
   "source": [
    "__Read, preprocess with Ray Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b27cdf-1429-406b-9b6d-fe66d42060bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ray.data.read_parquet(\"s3://anonymous@anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\").repartition(16)\n",
    "\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e412a-b244-40c1-9e41-a20cebd666e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Fit model with Ray Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cf09b-f72c-4476-a58a-caaf1193a27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    scaling_config=ScalingConfig(num_workers=4, use_gpu=False),\n",
    "    params={ \"objective\": \"binary:logistic\", },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140122e-c5d2-40b5-8832-ad4ecbd2b344",
   "metadata": {},
   "source": [
    "__Optimize hyperparams with Ray Tune__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f118c3d-569b-476a-88df-0d1bfa82d9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer, \n",
    "            param_space={'params' : {'max_depth': tune.randint(2, 12)}},\n",
    "            tune_config=TuneConfig(num_samples=3, metric='train-logloss', mode='min'))\n",
    "\n",
    "checkpoint = tuner.fit().get_best_result().checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66135c6-4faa-4257-a098-974b1edfce96",
   "metadata": {},
   "source": [
    "__Batch prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6603d8-a890-48b4-a26e-839329708486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n",
    "\n",
    "predicted_probabilities = batch_predictor.predict(valid_dataset.drop_columns(['is_big_tip']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e58aa-5eff-4ff9-9c7f-e3af74db75ce",
   "metadata": {},
   "source": [
    "__Online prediction with Ray Serve__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579107d9-9bb6-4223-bbd0-86d2c1dc60a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment = PredictorDeployment.bind(XGBoostPredictor, result.checkpoint, http_adapter=pandas_read_json)\n",
    "\n",
    "serve.run(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6c6ea-15c3-4337-823c-423980e2e38e",
   "metadata": {},
   "source": [
    "__HTTP or Python services__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9e200-74ff-4ecf-a36d-d0865ef3bae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = dict(valid_dataset.take(1)[0])\n",
    "del(sample_input['is_big_tip'])\n",
    "del(sample_input['__index_level_0__'])\n",
    "requests.post(\"http://localhost:8000/\", json=[sample_input]).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d86ad9-e61f-407a-ad5a-c085653f2095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc41d8f-4332-4359-b957-f26bc79de7a3",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "\n",
    "Serve is a microservices framework for serving ML – the model serving\n",
    "component of Ray AIR.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "# Deployments\n",
    "\n",
    "`Deployment` is the fundamental user-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "## Our First Service\n",
    "\n",
    "Let’s jump right in and get something simple up and running on Ray\n",
    "Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48953d75-e69a-4fe3-adf8-079edc953683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class TextConverter:\n",
    "    def convert(self, text):\n",
    "        return \"***\" + str.upper(text) + \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62641bcc-f2a0-48ee-ac1d-182688552af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_handle = serve.run(TextConverter.bind())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7ce21-ee2a-48cb-8043-e986a4522e89",
   "metadata": {},
   "source": [
    "## Key APIs and concepts\n",
    "\n",
    "`Deployment` represents a service and is created with the `@serve.deployment` decorator\n",
    "* As end users, we don't instantiate `Deployment`s directly\n",
    "* Ray will create them as actors, per our scaling requirements\n",
    "\n",
    "A __bound deployment__ is created with the `.bind` class method on the deployment class\n",
    "* e.g., `TextConverter.bind(msg=\"Yes...\")` above creates a bound deployment\n",
    "* `.bind` allows us to provide constructor params for the deployment class (the `msg` param above)\n",
    "* bound deployments *can* be passed to other deployments via `.bind` -- this is one way to compose services\n",
    "* We can pass a bound deployment to `serve.run(...)`\n",
    "    * to start a service\n",
    "    * to obtain a `ServeHandle`\n",
    "\n",
    "A `ServeHandle` can be used to invoke services through the Python API\n",
    "* At runtime, services can call other services via serve handles\n",
    "    * Bound deployments provided to deployment constructors via `.bind` become serve handles at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50986b5-b5d5-48e1-b7ce-d5fa27a2121e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(app_handle))\n",
    "print(app_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f5c3e-f185-4d7d-9f97-48a0ffee66dc",
   "metadata": {},
   "source": [
    "Look at Actors in the dashboard. Why are deployment replicas actors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085cc7d1-dad5-4c82-8073-2b1a44ecd2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_handle.convert.remote(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3303f-1a92-4e19-abd0-1b255aa7704c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(app_handle.convert.remote(\"cat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e0c5b-c19f-4c15-9792-42cae983d7e0",
   "metadata": {},
   "source": [
    "Ok, we have a minimal deployment built and running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167f6ee-55c1-469f-b945-8c37fce4c581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2d2f7-1076-4aff-aa52-7b08d36a323d",
   "metadata": {},
   "source": [
    "What do we want to do next?\n",
    "* Support some image processing\n",
    "* Support HTTP\n",
    "\n",
    "Then...\n",
    "* Image segmentation with SegFormer\n",
    "* Service composition -- e.g., grayscale/resize/sharpen/etc. and then segment\n",
    "\n",
    "And finally...\n",
    "* Manage GPUs (and resources generally)\n",
    "* Multiple replicas, autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15073d2c-f859-4454-a4ef-32b00943add1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Threshold:\n",
    "    def __init__(self, threshold: int):\n",
    "        self._threshold = threshold # initial state\n",
    "    \n",
    "    def get_response(self, image):\n",
    "        new_image = np.zeros_like(image)\n",
    "        new_image[image > self._threshold] = 255\n",
    "        new_image[new_image < 255] = 0\n",
    "        return new_image\n",
    "\n",
    "app_handle = serve.run(Threshold.bind(threshold=128), name='hello_image_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f06f9-08ea-484f-b832-3eb99bd76d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im = Image.open(\"cat.jpg\")\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df12907-1999-44df-b111-6f7f45d99681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im = im.resize((512,384))\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ca27c-4179-45bf-87c0-d031aff283c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(im.getdata()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a59531-8255-4432-a540-f7e29b23c1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = np.array(im.getdata())\n",
    "arr = arr.reshape(-1, 512, 3)\n",
    "\n",
    "plt.imshow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b923d3-c2f5-41d9-bd5b-2e51be1b6d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(arr.mean(axis=2), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a0b5e-1c26-4ce2-bff2-0d91afbf1f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ref = app_handle.get_response.remote(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6550f5-d977-4438-9f20-a68663f5ad3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ray.get(output_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701bd2e-1042-4d27-af9a-16f2b4261550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('hello_image_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40c36f-8d6c-4bed-83cb-759bd9f8c0e5",
   "metadata": {},
   "source": [
    "Add HTTP ... this is a bit messier just because of conversion between bytes, arrays, and HTTP tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa8d0-4d4a-41bd-b206-9de536da5c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Threshold:\n",
    "    def __init__(self, threshold: int):\n",
    "        self._threshold = threshold # initial state\n",
    "\n",
    "    def get_response(self, image):\n",
    "        new_image = np.zeros_like(image)\n",
    "        new_image[image > self._threshold] = 255\n",
    "        new_image[new_image < 255] = 0\n",
    "        return new_image\n",
    "    \n",
    "    # a lot of boilerplate as HTTP adapter for images + ndarrays (a text/JSON example would be about 3 lines)\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        import numpy as np\n",
    "        import io\n",
    "        from imageio import v3 as iio\n",
    "        from fastapi import Response\n",
    "\n",
    "        # async collect POST body\n",
    "        body = await request.body()\n",
    "        \n",
    "        # unpickle serialized data\n",
    "        image = pickle.loads(body)\n",
    "        \n",
    "        # get NDArray for our image processing\n",
    "        data = np.array(image)\n",
    "        \n",
    "        # invoke existing business logic\n",
    "        transformed_data = self.get_response(data)\n",
    "        \n",
    "        # convert to image\n",
    "        transformed_image = Image.fromarray(transformed_data.astype(np.uint8))\n",
    "        \n",
    "        # prepare output buffer\n",
    "        with io.BytesIO() as buf:\n",
    "            iio.imwrite(buf, transformed_image, plugin=\"pillow\", format=\"JPEG\")\n",
    "            im_bytes = buf.getvalue()\n",
    "        \n",
    "        # prepare and return HTTP Response\n",
    "        headers = {'Content-Disposition': 'inline'} # ; filename=\"test.jpeg\"'}\n",
    "        return Response(im_bytes, headers=headers, media_type='image/jpeg')\n",
    "\n",
    "app_handle = serve.run(Threshold.bind(threshold=128), name='hello_image_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd72e73-02f1-4a51-af72-ee7012b785eb",
   "metadata": {},
   "source": [
    "Threshold our cat via HTTP\n",
    "\n",
    "(if we are working with arrays on the client side and want to make an image from an array, we'd call `Image.fromarray(my_array)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b088b7-56b2-44be-bad1-ff780b835179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:8000/\", data = pickle.dumps(im)) # uncompressed\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582f1de-a173-4a88-8a18-e20d28c710be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8e3c2-a65e-4011-8da1-af48234735a5",
   "metadata": {},
   "source": [
    "## Build a semantic segmentation service on SegFormer\n",
    "\n",
    "At this point, we've done all the hard work -- we know the structure of our service code.\n",
    "\n",
    "In this use case, we're going to build and test\n",
    "* SegFormer-based segmentation service\n",
    "* Image prep service (as a demo, we'll just convert the image to grayscale, but feel free to experiment with other transformations)\n",
    "* an Ingress service, to separate the HTTP handling code from our other components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15aad6-c1dd-47ca-9960-021cc0aa4b08",
   "metadata": {},
   "source": [
    "### Segmentation service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f01a55-19e8-4835-9890-aad7e594b009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c656e-0439-44a2-81b9-641c3499f3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label, label2id = get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4cce0-6b75-46f9-a739-2070e844f3f8",
   "metadata": {},
   "source": [
    "### Load the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbfd65-422d-4797-924a-406047d371b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Segmenter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name, id2label, label2id,\n",
    "    ):\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name, id2label=id2label, label2id=label2id)\n",
    "        self.feature_extractor = SegformerFeatureExtractor.from_pretrained(model_name, do_reduce_labels=True)\n",
    "\n",
    "    def segment(self, batch: list) -> list[np.ndarray]: # can process PIL Image, or torch/np tensor\n",
    "\n",
    "        # Set the device on which PyTorch will run.\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)  # Move the model to specified device.\n",
    "        self.model.eval()  # Set the model in evaluation mode on test data.\n",
    "\n",
    "        # The feature extractor processes raw images.\n",
    "        inputs = self.feature_extractor(images=batch, return_tensors=\"pt\")\n",
    "\n",
    "        # The model is applied to input images in the inference step.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        # Post-process the output for display.\n",
    "        image_sizes = [image.size[::-1] for image in batch]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Return list of segmentation maps detached from the computation graph.\n",
    "        return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997f2f6-e19f-4b83-a0a7-7a74eee4bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter.bind(\"nvidia/segformer-b0-finetuned-ade-512-512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782dd94b-e65f-4b83-84f1-d330b0724ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('hello_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430d38e-60fb-4a6d-bad1-43e36ca05a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handle = serve.run(segmenter, name='seg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4a4cf-442c-4c04-9650-0a9a37e0d0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = handle.segment.remote([im])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b3f32-4582-49aa-8f8c-051df293d9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ray.get(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e217333-e681-4b88-88ee-0205f3d07ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im.convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd2a73-8603-443d-8456-30a4fc567484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87739f5-3fa7-4ccf-afdf-6ac8db50548a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out2 = handle.segment.remote([im.resize((360,240))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7d6f5-6e1b-4877-aeea-0f82991184ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ray.get(out2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272231a-d378-4ada-98d1-32fddd6439b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im = Image.open(\"./people-room.jpg\")\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232eac6a-d768-4c47-bf3c-b786aa4d2894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50107ec5-1fb2-43b1-9c2e-2b9785cca394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccc0fa-1b51-4dbb-8cf8-b8b1db048ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b390c1-38fb-40fc-b81f-8b5fcd2c378c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = handle.get_response.remote('hello')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca7ca-3c5d-4cb8-9e8e-88d98b2a1640",
   "metadata": {},
   "source": [
    "In order to support maximal performance, values from remote calls, such as our response string here, are returned as object references (a bit like futures or promises in some frameworks). If we want to block, wait for the result to be ready, and retrieve it, we can use `ray.get(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142e154-6313-4eee-acf9-ea7e640ed78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346a23e-b29f-4f0a-847a-989e00640f41",
   "metadata": {},
   "source": [
    "Since we'll be creating a new application example, we can delete the old one -- that allows Ray to remove the replicas of our Chat deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c62ae7-8739-45cf-b839-f3a22b4463b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('hello_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a4917-c20b-4da2-b27c-0517b99e27ce",
   "metadata": {},
   "source": [
    "## Specifying service resources\n",
    "\n",
    "Resources can be specified on a per-deployment basis and, if we want, in fractional units, via the `ray_actor_options` parameter on the `@serve.deployment` decorator.\n",
    "\n",
    "As a realistic example, we can upgrade the \"hello world\" chatbot to use a Huggingface LLM employing GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f88fe-73db-41b7-a0d0-df0e454c21d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5})\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        # configure stateful elements of our service such as loading a model\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        # path to handle HTTP requests\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        # after decoding the payload, we delegate to get_response for logic\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        # this method receives calls directly (from Python) or from __call__ (from HTTP)\n",
    "        history.append(user_input)\n",
    "        # the history is client-side state and will be a list of raw strings;\n",
    "        # for the default config of the model and tokenizer, history should be joined with '</s><s>'\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae819eab-26bf-409c-92de-7f6a8c79cf65",
   "metadata": {
    "tags": []
   },
   "source": [
    "Resources can include\n",
    "* `num_cpus`\n",
    "* `num_gpus`\n",
    "* `resources` dictionary containing custom resources\n",
    "    * custom resources are tracked and accounted as symbols (or tags) in order to match actors to workers\n",
    "    \n",
    "Example\n",
    "```python\n",
    "@serve.deployment(ray_actor_options={'num_cpus' : 2, 'num_gpus' : 2, resources : {\"my_super_accelerator\": 1}})\n",
    "class Demo:\n",
    "    ...\n",
    "```\n",
    "\n",
    "The purpose of the declarative resource mechanism is to allow Ray to place code on suitable nodes in a heterogeneous cluster without our having know which nodes have which resources to where our code should run.\n",
    "\n",
    "> Best practice: if some nodes have a distinguising feature, mark and request it as a resource, rather than trying to determine which nodes are present and where your code will run.\n",
    "\n",
    "For more details, see https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#resource-management-cpus-gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a4535-fdf4-4bb1-858f-203a20eb7afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "handle = serve.run(chat, name='basic_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bb616-f1ab-4398-8748-d6ff78058cca",
   "metadata": {},
   "source": [
    "### Runtime environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db24df1-13f6-4f69-b11e-1119be71e6a5",
   "metadata": {},
   "source": [
    "We have many options for managing dependencies -- e.g., Python libraries and versions, resource file, etc.\n",
    "\n",
    "Dependencies can be provided at the level of Node/VM/container, Ray jobs, actors, tasks, and more.\n",
    "\n",
    "With Ray Serve, we can optionally specify environment requirements at the `Deployment` level, and Ray will ensure that the specified environment is available to that deployment.\n",
    "\n",
    "In the following example, we'll create \n",
    "* some services that use libraries available in our general Ray enviornment\n",
    "* a service that requires a specific Python library (a language detector library) to illustrate the environment feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c808e-62ec-4d7b-ad57-fb31d1ac32b9",
   "metadata": {},
   "source": [
    "Since we are discussing dependencies, its important to remember that it's a good practice to keep as many dependencies as possible in our general Ray worker environments, and to import them as usual.\n",
    "\n",
    "> Just because we *can* create lots of custom environments in our code doesn't mean we *should*\n",
    "\n",
    "In this first service, we import `pipeline` from Huggingface transformers. Later, the specific pipeline we need will require `sentencepiece`. We'll demo installing `sentencepiece` via the Runtime Environment. \n",
    "\n",
    "Beyond just specifying the library, we have to be careful about the order of imports and other calls, to ensure we don't need something from the library before it's available. We ensure that by delaying imports or use of anything with a relevant import until an actual method is called on our service. We can capture variables as usual in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b992cb-3752-4cf9-b6d7-18ccb7b69300",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358,
     "referenced_widgets": [
      "ed182dd854d1439ab6d44e39210d7074",
      "4ed3f579be5d450aacfa0f970b8d81da",
      "5f0dc2de13284933ae2439af1fe88753",
      "3d98bfe346ed40a8bd5d46de450bca24",
      "1737ac3e202a41688c4adb0b498ec58f",
      "3f127b730e024ffa91ab921ac5291c73",
      "bbd6c25a5bef464fa74837811f1e3a39",
      "442b5bbd50f145e9844495dcda8c09d5",
      "13efed5b0c204ce58120470ae37e6af2",
      "5f28f3c322484ff5a88e988790417d28",
      "d5ff21c53b644abbaa8c2a06c38d2d1d",
      "9860b04b1772493e819f80ceb1000aa0",
      "506f934b75c84618a49f92188d1520b3",
      "4de12f5510974cd79848308c361db070",
      "5b28b54f06f7445aaeab92194bc5951f",
      "eb16d41e87a14b18a33ce16aab9c9c8c",
      "ecd2111b9e974b3eaf712610ac19bb9c",
      "cc0c31d2e0f3494ca2d0440cf5d030d5",
      "7635b8cd4b0748e784b264301b94e535",
      "2b2393d742b344f5b1f79424dec2c4ca",
      "5f8fc5621284480bb1ca39ec6ddf7ff9",
      "13a316e0e13c492b8c9a7b7b3ffdad99",
      "9e8c49cfece042298d111c9dcd5d8831",
      "dd2b9a3f127a445995d25db6058fa425",
      "c2a59a1b016e4e4eabe47c41641d3322",
      "71d0e0924bba466f8cabc03b0a050b88",
      "ef16c90488864b088aa2f402cb3dbdf7",
      "ba44f27002c34b248ed4f70e1ac6f74d",
      "ea43a14dd52e45038a3746743ab3faac",
      "40b0e1c0bc2a40b08e4f46ab95fd5806",
      "465b13292f2b40839211874c8c7c0aee",
      "b707f3785b2d407db18f937c53d3cd65",
      "384044f6a71b46c8bcf6495c6797c74c",
      "38b584bc5a984778868f19a690a18529",
      "bd8e3a9d758140ca88d08f95fc3d0e58",
      "36b7159684c24fa18f5b250d57ef370e",
      "6431501c8c424881855cd372d2b37e77",
      "81c37bcc6d684b47841805ed43c0d8aa",
      "a710d60f9cfa405b8de2f9658c65a24f",
      "a448c57bdb41434295c745f66c0639e6",
      "6ae7d8496ac747e8989a8047775ba099",
      "0fa581a4f548419a9d7ed301a89c9712",
      "49cbb21b99d94a37b290943c926b1410",
      "c479d3b81e4e4abcae5351bb1737155d",
      "6a04740303ed4265907db573a01dd5dc",
      "90b52d08441e4366a03ce5d943d4b3c3",
      "12b42905d56947b2949ebfe2b202fe18",
      "c723559e7e9b420bbf4c417625631647",
      "49fe982496a64136a5a858a6191f1254",
      "0bb83c7a35754c46b704c68b95cf13d9",
      "053a38fae8664d7a9f509fd6f9b13b35",
      "771cd557b6ce42a586878d2d05568e89",
      "12b77bb22ca943bb9fd842278b6b135f",
      "1373256b833b402b8c2e28ab825537a5",
      "1970ae7144244e3eb650bc94f7b1c209"
     ]
    },
    "id": "d55d5f93-b22c-4a76-9261-290862c62882",
    "outputId": "ff6a42ad-fe0a-4b65-f289-e610536baa1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_env = {\"pip\": [\"sentencepiece\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff8921-cb86-49e8-9ed3-6594ecb9095a",
   "metadata": {
    "id": "d1b79e65-6a72-4046-a551-dea32116b83d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : runtime_env})\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._task = task\n",
    "        self._model = model\n",
    "        self._pipeline = None\n",
    "    \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        if (self._pipeline is None):\n",
    "            self._pipeline = pipeline(task=self._task, model=self._model)\n",
    "        outputs = self._pipeline(user_input)\n",
    "        response = outputs[0]['translation_text']\n",
    "        return response\n",
    "        \n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86d7e7-509f-4d57-9856-24a289cd1926",
   "metadata": {},
   "source": [
    "Notice how we have two different services but they are built on the same reusable code by calling `.bind()` with different initialization parameters.\n",
    "\n",
    "*We don’t need to define new deployments for every service we use.*\n",
    "\n",
    "This time we’re haven't published an application (via `serve.run()`) because these components will be invoked only by our main service deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347781f-13ce-468f-a3f7-06aa4262d4a6",
   "metadata": {
    "id": "778ede38-d221-4f45-8e2d-71ec0702aae4",
    "tags": []
   },
   "source": [
    "## Composition patterns\n",
    "\n",
    "Let's bring the whole system together. We'll implement a service which represents our external endpoint for HTTP or Python invocations.\n",
    "* This service will have references to the deployments we've built so far, and will implement some conditional logic to ensure the correct language is used\n",
    "* Note that even if the user is interacting in French, we need to return the English response as well so that client can use that to build the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bfdad-f7d6-4f04-be2b-f64963a80084",
   "metadata": {},
   "source": [
    "### Imperative pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717ca2e-d6c6-4ef3-975d-fe34c9919a84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22f85caa-133e-4df7-8aab-4e3278ffc54b",
    "outputId": "2086ccc1-0d59-419f-d47f-d6144980f536",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Endpoint:\n",
    "    def __init__(self, chat, lang_detect, translate_en_fr, translate_fr_en):\n",
    "        # assign dependent service handles to instance variables\n",
    "        self._chat = chat\n",
    "        self._lang_detect = lang_detect\n",
    "        self._translate_en_fr = translate_en_fr\n",
    "        self._translate_fr_en = translate_fr_en\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {'response': await self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    async def get_response(self, user_input: str, history: list[str]):\n",
    "        lang_obj_ref = await self._lang_detect.get_response.remote(user_input)\n",
    "        \n",
    "        # if we didn't need the literal value of the language yet, we could pass that (future) object reference to other services\n",
    "        # here, though, we need the value in order to decide whether to call the translation services\n",
    "        # we get the Python value by awaiting the object reference\n",
    "        lang = await lang_obj_ref\n",
    "\n",
    "        if (lang == 'fr'):\n",
    "            user_input = await self._translate_fr_en.get_response.remote(user_input)\n",
    "\n",
    "        response = response_en = await self._chat.get_response.remote(user_input, history)\n",
    "        \n",
    "        if (lang == 'fr'):\n",
    "            response = await self._translate_en_fr.get_response.remote(response_en)\n",
    "            user_input = await user_input\n",
    "            \n",
    "        response = await response\n",
    "        response_en = await response_en\n",
    "        \n",
    "        return response  + '|' + user_input + '|' + response_en\n",
    "\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "endpoint = Endpoint.bind(chat, lang_detect, translate_en_fr, translate_fr_en)\n",
    "\n",
    "endpoint_handle = serve.run(endpoint, name = 'multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f16ab-1f13-4d0d-a33d-d52f6c266782",
   "metadata": {},
   "source": [
    "We've implemented control flow through our services and used the async/await pattern in several places so that we don't unnecessarily block.\n",
    "\n",
    "Then we construct the service endpoint and start a new application serving that endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63165f18-216a-411f-99f1-4fb55dd8aa8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "fbe6a39a-1b55-49c6-8a7e-632256588bf3",
    "outputId": "fd81afe6-5f31-4777-ecd5-c4d138ca901e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'My friends are cool but they eat too many carbs.'\n",
    "history = []\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35872a30-9ed0-4ac9-b11d-1cab798a5cf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b852c27-20d3-4cc3-b300-5fd15e0ba124",
    "outputId": "386cf1b6-daeb-4a19-d0cd-248131bd9f52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c90bec-1b5e-4a1a-b67a-1e11f37169ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "73855166-c226-40f9-a365-babb36f751b3",
    "outputId": "17498373-04e4-4595-80af-88cb24c568f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'Je ne suis pas sûr.'\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402694c-e4a3-4227-b438-0faab9dd9e3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25952d4a-43fe-49fa-a437-eb91813cde5c",
    "outputId": "29e847c0-4819-46c6-d15c-681bab01e028",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d25201-b26a-43c1-b8e7-0f6f5fc9fea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this point we have a service which can support the many functional and operational properties we expect to need in production, including scalability, separation of concerns, and composability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6bab4-7552-4ab5-9439-0df07d632fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48a6a1-4f60-4ca7-889a-f31866cbe4c7",
   "metadata": {
    "id": "6eae86f3-34a4-4f59-87ba-df8d9a0d3928"
   },
   "source": [
    "### Declarative pattern: Deployment Graph API\n",
    "\n",
    "What is the Deployment Graph API?\n",
    "\n",
    "* The Deployment Graph API lets us separate the flow of calls from the logic inside our services.\n",
    "\n",
    "Why might we want to use the Deployment Graph (DAG) API to separate flow from logic?\n",
    "\n",
    "* It may be valuable to add a layer of indirection – or abstraction – so that we can more easily create and compose reusable services\n",
    "* The DAG API lets us use similar patterns across the Ray platform (e.g., Ray Workflow)\n",
    "    * We can learn one general pattern for graphs and use that intuition in multiple places in our Ray applications\n",
    "* Although we compose one DAG, we retain the key Ray Serve features of granular autoscaling and resource allocation\n",
    "\n",
    "Let’s reproduce our chat service flow using the Deployment Graph API\n",
    "\n",
    "#### Getting started with deployment graphs\n",
    "\n",
    "As a first step, to keep things simple, let’s assume for a moment that we are always interacting with the service in French. \n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_simple.png' width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9def2-6de2-43a8-a465-aa3ea2478fba",
   "metadata": {
    "id": "3500503d-0831-47ae-88e0-88b8a8a7e2cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.serve.dag import InputNode\n",
    "from ray.serve.drivers import DAGDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9625b-4e2b-4d72-b314-0580096fb311",
   "metadata": {},
   "source": [
    "`InputNode` is a special type of graph node, defined by Ray Serve, which represents values supplied to our service endpoint. \n",
    "\n",
    "We can only have one `InputNode` but we can get access to multiple parameters from that node using a Python context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca019007-7f86-4b97-a0f0-60427bfeae8f",
   "metadata": {
    "id": "867cba3f-7d6e-4593-9dac-49f1069c93b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc54441-64a2-46e0-a105-c35298153bf1",
   "metadata": {
    "id": "d2463417-b8c8-41ee-a259-7708d3952ff8"
   },
   "source": [
    "Here is a minimal, linear pipeline that allows us to begin a chat in French.\n",
    "\n",
    "We build up the graph step by step, `bind`ing each deployment to its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f855710-5c45-43e0-8446-97d231cd5ac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6552ef01-a62b-497f-b578-8fc20edc6000",
    "outputId": "ad1eb72e-6c8c-473f-a92f-89d5bbebdeb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input_en = translate_fr_en.get_response.bind(user_input)    # French->English translator depends on the user input text\n",
    "chat_response = chat.get_response.bind(user_input_en, history)   # the chat deployment requires the English user input and the history\n",
    "output = translate_en_fr.get_response.bind(chat_response)        # English->French translator depends on the English chat output\n",
    "serve_dag = DAGDriver.bind(output)                               # the graph returns the output from the English->French translator\n",
    "\n",
    "handle = serve.run(serve_dag, name='basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f463b-d98a-4a55-a2ee-2c5ed0c3a2ed",
   "metadata": {},
   "source": [
    "We start the application by calling `serve.run()` on the DAGDriver, a Ray Serve component which routes HTTP requests through your call graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90e16c-9ea9-4e24-a681-7a86ce519014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "5e740b79-0e42-4883-9b38-ab3976e8f160",
    "outputId": "28adadf4-9a78-44ae-e235-9f1ddf233a75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Mes amis sont cool mais ils mangent trop de glucides.', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df0103-07f6-45c3-a643-472930f26d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb75ac-aa59-4d69-9740-a44e61d13e08",
   "metadata": {},
   "source": [
    "## Architecture / under-the-hood\n",
    "\n",
    "### Ray cluster perspective: actors\n",
    "\n",
    "In Ray, user code is executed by worker processes. These workers can run tasks (stateless functions) or actors (stateful class instances).\n",
    "\n",
    "Ray Serve is built on actors, allowing deployments to collect expensive state once (such as loading a ML model) and to reuse it across many service requests.\n",
    "\n",
    "Although you may never need to code any Ray tasks or actors yourself, your Ray Serve application has full access to those cluster capabilities and you may wish to use them to implement other functionality (e.g., service or operations that don't need to accept HTTP traffic). More information is at https://docs.ray.io/en/latest/ray-core/walkthrough.html\n",
    "\n",
    "### Serve design\n",
    "\n",
    "Under the hood, a few other actors are used to make up a serve instance.\n",
    "\n",
    "* Controller: A global actor unique to each Serve instance is responsible for managing other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "* HTTP Proxy: By default there is one HTTP proxy actor on the head node that accepts incoming requests, forwards them to replicas, and responds once they are completed. For scalability and high availability, you can also run a proxy on each node in the cluster via the location field of http_options.\n",
    "\n",
    "* Deployment Replicas: Actors that execute the code in response to a request. Each replica processes requests from the HTTP proxy.\n",
    "<img src='https://docs.ray.io/en/latest/_images/architecture-2.0.svg' width=700 />\n",
    "\n",
    "Incoming requests, once resolved to a particular deployment, are queued. The requests from the queue are assigned round-robin to available replicas as long as capacity is available. This design provides load balancing and elasticity. \n",
    "\n",
    "Capacity can be managed with the `max_concurrent_queries` parameter to the deployment decorator. This value defaults to 100 and represents the maximum number of queries that will be sent to a replica of this deployment without receiving a response. Each replica has its own queue to collect and smooth incoming request traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e217e9-ba60-4172-858b-65fd5fb03359",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling and performance\n",
    "\n",
    "### Replicas and autoscaling\n",
    "\n",
    "Each deployment can have its own resource management and autoscaling configuration, with several options for scaling.\n",
    "\n",
    "By default -- if nothing specified, as in our examples above -- the default is a single. We can specify a larger, constant number of replicas in the decorator:\n",
    "```python\n",
    "@serve.deployment(num_replicas=3)\n",
    "```\n",
    "\n",
    "For autoscaling, instead of `num_replicas`, we provide an `autoscaling_config` dictionary. With autoscaling, we can specify a minimum and maximum range for the number of replicas, the initial replica count, a load target, and more.\n",
    "\n",
    "Here is example of extended configuration -- see https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation for more details:\n",
    "\n",
    "```python\n",
    "@serve.deployment(\n",
    "    autoscaling_config={\n",
    "        'min_replicas': 1,\n",
    "        'initial_replicas': 2,\n",
    "        'max_replicas': 5,\n",
    "        'target_num_ongoing_requests_per_replica': 10,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "`min_replicas` can also be set to zero to create a \"serverless\" style design: in exchange for potentially slower startup, no actors (or their CPU/GPU resources) need to be permanently reserved.\n",
    "\n",
    "### Autoscaling LLM chat\n",
    "\n",
    "The LLM-baset chat service is a good example for seeing autoscaling in action, because LLM inference is relative expensive so we can easily build up a queue of requests to the service. The autoscaler responds to the dynamics of queue sizes and will launch additional replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101cbf7-8ac2-46fa-8684-682666f566cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbde82-e3ab-486e-a0e3-1112d31e6fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5}, autoscaling_config={ 'min_replicas': 1, 'max_replicas': 4 })\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        history.append(user_input)\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response\n",
    "    \n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "handle = serve.run(chat, name='autoscale_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700ec18-fbcb-4db2-98cb-5d6a8ff9f244",
   "metadata": {},
   "source": [
    "We can generate a little load and look at the Ray Dashboard\n",
    "\n",
    "What do we expect to see?\n",
    "\n",
    "* Autoscaling of the Chat service up to 4 replicas\n",
    "* Efficient use of fractional GPU resources\n",
    "    * If our cluster has just 2 GPUs, we can run 4 replicase there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000403d0-8073-418d-9e57-8177025d20ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_request(s):\n",
    "    return requests.post(\"http://localhost:8000/\", json = s).json()\n",
    "\n",
    "sample = '{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'\n",
    "make_request(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d0eba-b03d-4082-b301-16d29bbfe2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "results = executor.map(make_request, ['{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'] * 40)\n",
    "\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954e172-1e1e-4c73-9d65-2886a8c3271c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('autoscale_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cae4aa-7c84-40fa-8f40-1cbd8bd64445",
   "metadata": {},
   "source": [
    "### Request batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557d733-aa66-4f12-9b01-8fefdd1a4128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment()\n",
    "class Chat:\n",
    "    def __init__(self):\n",
    "        self._message = \"Chatbot counts the batch size at \"\n",
    "\n",
    "    @serve.batch(max_batch_size=5)\n",
    "    async def handle_batch(self, request_batch):\n",
    "        num_requests = len(request_batch)\n",
    "        return [ {'response': self._message + str(num_requests) } ] * num_requests\n",
    "    \n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return await self.handle_batch(data)\n",
    "    \n",
    "chat = Chat.bind()\n",
    "\n",
    "handle = serve.run(chat, name='batch_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808017b-4aee-4bc4-b188-2e6311cca439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = executor.map(make_request, ['{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576e111-35da-48c2-84c4-4bea3544fb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d56dd1-46b7-45da-9a33-10deb32c20fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('batch_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc07e1-4574-419d-adb2-ca593d471963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
