{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399be64-44d3-45e4-9b43-3757a9b92daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray import tune\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f391603-346b-4f08-a5b9-dee050b77409",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Outline\n",
    "\n",
    "-   Deployments\n",
    "    -   Resources (CPU/GPU/custom)\n",
    "    -   Runtime environments support, usage (functionality)\n",
    "    -   Bound deployments, ServeHandles\n",
    "-   Composition Patterns\n",
    "    -   Imperative\n",
    "    -   Declarative / Graph Deployment API\n",
    "-   Architecture / Under-the-hood\n",
    "    -   Ray cluster perspective - processes / workers / actors\n",
    "    -   Request routing, queuing, load balancing in Serve\n",
    "-   Scaling and Performance\n",
    "    -   Replicas\n",
    "        -   num_replicas, autoscaling_config, max_concurrent_queries\n",
    "    -   Request batching\n",
    "\n",
    "### Example scenario: multilingual LLM chat\n",
    "\n",
    "For our example use case, we’ll see how to leverage Ray Serve to host a LLM Chat\n",
    "model and how to enhance it using additional services for multilingual\n",
    "interactions.\n",
    "\n",
    "### Context: Ray AIR\n",
    "\n",
    "Ray AIR is the Ray AI Runtime, a set of high-level easy-to-use APIs for\n",
    "ingesting data, training models – including reinforcement learning\n",
    "models – tuning those models and then serving them.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=600 loading=\"lazy\"/>\n",
    "\n",
    "Key principles behind Ray and Ray AIR are\n",
    "* Performance\n",
    "* Developer experience and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf588b5-4b9a-4c2e-9ac0-f7b23213579a",
   "metadata": {},
   "source": [
    "__Read, preprocess with Ray Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b27cdf-1429-406b-9b6d-fe66d42060bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ray.data.read_parquet(\"s3://anonymous@anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\").repartition(16)\n",
    "\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e412a-b244-40c1-9e41-a20cebd666e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Fit model with Ray Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cf09b-f72c-4476-a58a-caaf1193a27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    scaling_config=ScalingConfig(num_workers=4, use_gpu=False),\n",
    "    params={ \"objective\": \"binary:logistic\", },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140122e-c5d2-40b5-8832-ad4ecbd2b344",
   "metadata": {},
   "source": [
    "__Optimize hyperparams with Ray Tune__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f118c3d-569b-476a-88df-0d1bfa82d9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer, \n",
    "            param_space={'params' : {'max_depth': tune.randint(2, 12)}},\n",
    "            tune_config=TuneConfig(num_samples=3, metric='train-logloss', mode='min'))\n",
    "\n",
    "checkpoint = tuner.fit().get_best_result().checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66135c6-4faa-4257-a098-974b1edfce96",
   "metadata": {},
   "source": [
    "__Batch prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6603d8-a890-48b4-a26e-839329708486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n",
    "\n",
    "predicted_probabilities = batch_predictor.predict(valid_dataset.drop_columns(['is_big_tip']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e58aa-5eff-4ff9-9c7f-e3af74db75ce",
   "metadata": {},
   "source": [
    "__Online prediction with Ray Serve__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579107d9-9bb6-4223-bbd0-86d2c1dc60a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment = PredictorDeployment.bind(XGBoostPredictor, result.checkpoint, http_adapter=pandas_read_json)\n",
    "\n",
    "serve.run(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6c6ea-15c3-4337-823c-423980e2e38e",
   "metadata": {},
   "source": [
    "__HTTP or Python services__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9e200-74ff-4ecf-a36d-d0865ef3bae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = dict(valid_dataset.take(1)[0])\n",
    "del(sample_input['is_big_tip'])\n",
    "del(sample_input['__index_level_0__'])\n",
    "requests.post(\"http://localhost:8000/\", json=[sample_input]).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d86ad9-e61f-407a-ad5a-c085653f2095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc41d8f-4332-4359-b957-f26bc79de7a3",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "\n",
    "Serve is a microservices framework for serving ML – the model serving\n",
    "component of Ray AIR.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "# Deployments\n",
    "\n",
    "`Deployment` is the fundamental user-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "## Our First Service\n",
    "\n",
    "Let’s jump right in and get something simple up and running on Ray\n",
    "Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa8d0-4d4a-41bd-b206-9de536da5c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Chat:\n",
    "    def __init__(self, msg: str):\n",
    "        self._msg = msg # initial state\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {\"result\": self.get_response(data['input']) }\n",
    "    \n",
    "    def get_response(self, message: str) -> str:\n",
    "        return self._msg + message\n",
    "\n",
    "handle = serve.run(Chat.bind(msg=\"Yes... \"), name='hello_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed55cd-4d79-40a6-8737-09ac0fc33782",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint or by invoking directly from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffe466-169f-4d73-9e3c-019d8c62fc2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_json = '{ \"input\" : \"hello\" }'\n",
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40134f3-a5ba-4554-95f9-74a7f7731e66",
   "metadata": {},
   "source": [
    "## Key APIs and concepts\n",
    "\n",
    "`Deployment` represents a service and is created with the `@serve.deployment` decorator\n",
    "* As end users, we don't instantiate `Deployment`s directly\n",
    "* Ray will create them as actors, per our scaling requirements\n",
    "\n",
    "A __bound deployment__ is created with the `.bind` class method on the deployment class\n",
    "* e.g., `Chat.bind(msg=\"Yes...\")` above creates a bound deployment\n",
    "* `.bind` allows us to provide constructor params for the deployment class (the `msg` param above)\n",
    "* bound deployments *can* be passed to other deployments via `.bind` -- this is one way to compose services\n",
    "* We can pass a bound deployment to `serve.run(...)`\n",
    "    * to start a service\n",
    "    * to obtain a `ServeHandle`\n",
    "\n",
    "A `ServeHandle` can be used to invoke services through the Python API\n",
    "* At runtime, services can call other services via serve handles\n",
    "    * Bound deployments provided to deployment constructors via `.bind` become serve handles at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50986b5-b5d5-48e1-b7ce-d5fa27a2121e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(handle))\n",
    "print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b390c1-38fb-40fc-b81f-8b5fcd2c378c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = handle.get_response.remote('hello')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca7ca-3c5d-4cb8-9e8e-88d98b2a1640",
   "metadata": {},
   "source": [
    "In order to support maximal performance, values from remote calls, such as our response string here, are returned as object references (a bit like futures or promises in some frameworks). If we want to block, wait for the result to be ready, and retrieve it, we can use `ray.get(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142e154-6313-4eee-acf9-ea7e640ed78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346a23e-b29f-4f0a-847a-989e00640f41",
   "metadata": {},
   "source": [
    "Since we'll be creating a new application example, we can delete the old one -- that allows Ray to remove the replicas of our Chat deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c62ae7-8739-45cf-b839-f3a22b4463b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('hello_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a4917-c20b-4da2-b27c-0517b99e27ce",
   "metadata": {},
   "source": [
    "## Specifying service resources\n",
    "\n",
    "Resources can be specified on a per-deployment basis and, if we want, in fractional units, via the `ray_actor_options` parameter on the `@serve.deployment` decorator.\n",
    "\n",
    "As a realistic example, we can upgrade the \"hello world\" chatbot to use a Huggingface LLM employing GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f88fe-73db-41b7-a0d0-df0e454c21d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5})\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        # configure stateful elements of our service such as loading a model\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        # path to handle HTTP requests\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        # after decoding the payload, we delegate to get_response for logic\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        # this method receives calls directly (from Python) or from __call__ (from HTTP)\n",
    "        history.append(user_input)\n",
    "        # the history is client-side state and will be a list of raw strings;\n",
    "        # for the default config of the model and tokenizer, history should be joined with '</s><s>'\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae819eab-26bf-409c-92de-7f6a8c79cf65",
   "metadata": {
    "tags": []
   },
   "source": [
    "Resources can include\n",
    "* `num_cpus`\n",
    "* `num_gpus`\n",
    "* `resources` dictionary containing custom resources\n",
    "    * custom resources are tracked and accounted as symbols (or tags) in order to match actors to workers\n",
    "    \n",
    "Example\n",
    "```python\n",
    "@serve.deployment(ray_actor_options={'num_cpus' : 2, 'num_gpus' : 2, resources : {\"my_super_accelerator\": 1}})\n",
    "class Demo:\n",
    "    ...\n",
    "```\n",
    "\n",
    "The purpose of the declarative resource mechanism is to allow Ray to place code on suitable nodes in a heterogeneous cluster without our having know which nodes have which resources to where our code should run.\n",
    "\n",
    "> Best practice: if some nodes have a distinguising feature, mark and request it as a resource, rather than trying to determine which nodes are present and where your code will run.\n",
    "\n",
    "For more details, see https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#resource-management-cpus-gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a4535-fdf4-4bb1-858f-203a20eb7afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "handle = serve.run(chat, name='basic_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f2cad-1123-4607-8f02-8873ec2609d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'My friends are cool but they eat too many carbs.'\n",
    "history = []\n",
    "response_handle = handle.get_response.remote(message, history)\n",
    "response = ray.get(response_handle)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca11a1-e4d7-450e-844a-367504a98f05",
   "metadata": {},
   "source": [
    "We prepare a message and a chat history list and call our chat service via Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5b70e-df82-4f9d-84b5-3e27887e87b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34549288-e4a2-4fbb-b41f-e7ea70f51d13",
    "outputId": "8c682533-9ebd-4965-dc75-b0661cf1f3d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += [message, response]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959662cb-cdd0-4d02-9fbb-572c8735b996",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "e1b448ae-3662-4211-bde6-72734f80ad30",
    "outputId": "28f4ea8b-2ce7-4aee-d1df-5546a74a5f9b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = \"I'm not sure.\"\n",
    "response_handle = handle.get_response.remote(message, history)\n",
    "response = ray.get(response_handle)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818e8bd-cdba-4fe5-84b4-3c62f2a2358f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('basic_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb611ab-85d7-40c6-b1fd-45a3c4e2144e",
   "metadata": {
    "id": "b54a5b65-7b86-4db0-821c-f3f9ce143316",
    "tags": []
   },
   "source": [
    "## Composing services with Ray for chatbot en Français: roadmap for additional Services\n",
    "\n",
    "The underlying chatbot model we’ve used only supports English interaction. But we can use the following recipe to add French language support:\n",
    "\n",
    "1. Implement a translation service between French and English\n",
    "1. Implement a language detection service\n",
    "1. Implement a routing (dispatch) service:\n",
    "    1. If the incoming prompt is French, then\n",
    "        1. Route the inbound prompt through the FR-EN translator\n",
    "        1. Pass the EN prompt to the chat model\n",
    "        1. Pass the EN output from the chat model through the EN-FR translator\n",
    "        1. Return the French response\n",
    "    1. Otherwise (if the prompt is in English), pass it straight to the chatbot as we did earlier and return the (English) response\n",
    "\n",
    "Let’s look using Ray Serve to implement model inference with these composed and conditional-flow elements using Python method calls (https://docs.ray.io/en/latest/serve/key-concepts.html#servehandle-composing-deployments). Later we’ll look at an alternative approach using Ray’s Deployment Graph API.\n",
    "\n",
    "We’ll implement parts 1 and 2 first…\n",
    "\n",
    "## Translation service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bb616-f1ab-4398-8748-d6ff78058cca",
   "metadata": {},
   "source": [
    "### Runtime environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db24df1-13f6-4f69-b11e-1119be71e6a5",
   "metadata": {},
   "source": [
    "We have many options for managing dependencies -- e.g., Python libraries and versions, resource file, etc.\n",
    "\n",
    "Dependencies can be provided at the level of Node/VM/container, Ray jobs, actors, tasks, and more.\n",
    "\n",
    "With Ray Serve, we can optionally specify environment requirements at the `Deployment` level, and Ray will ensure that the specified environment is available to that deployment.\n",
    "\n",
    "In the following example, we'll create \n",
    "* some services that use libraries available in our general Ray enviornment\n",
    "* a service that requires a specific Python library (a language detector library) to illustrate the environment feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c808e-62ec-4d7b-ad57-fb31d1ac32b9",
   "metadata": {},
   "source": [
    "Since we are discussing dependencies, its important to remember that it's a good practice to keep as many dependencies as possible in our general Ray worker environments, and to import them as usual.\n",
    "\n",
    "> Just because we *can* create lots of custom environments in our code doesn't mean we *should*\n",
    "\n",
    "In this first service, we import `pipeline` from Huggingface transformers. Later, the specific pipeline we need will require `sentencepiece`. We'll demo installing `sentencepiece` via the Runtime Environment. \n",
    "\n",
    "Beyond just specifying the library, we have to be careful about the order of imports and other calls, to ensure we don't need something from the library before it's available. We ensure that by delaying imports or use of anything with a relevant import until an actual method is called on our service. We can capture variables as usual in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b992cb-3752-4cf9-b6d7-18ccb7b69300",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358,
     "referenced_widgets": [
      "ed182dd854d1439ab6d44e39210d7074",
      "4ed3f579be5d450aacfa0f970b8d81da",
      "5f0dc2de13284933ae2439af1fe88753",
      "3d98bfe346ed40a8bd5d46de450bca24",
      "1737ac3e202a41688c4adb0b498ec58f",
      "3f127b730e024ffa91ab921ac5291c73",
      "bbd6c25a5bef464fa74837811f1e3a39",
      "442b5bbd50f145e9844495dcda8c09d5",
      "13efed5b0c204ce58120470ae37e6af2",
      "5f28f3c322484ff5a88e988790417d28",
      "d5ff21c53b644abbaa8c2a06c38d2d1d",
      "9860b04b1772493e819f80ceb1000aa0",
      "506f934b75c84618a49f92188d1520b3",
      "4de12f5510974cd79848308c361db070",
      "5b28b54f06f7445aaeab92194bc5951f",
      "eb16d41e87a14b18a33ce16aab9c9c8c",
      "ecd2111b9e974b3eaf712610ac19bb9c",
      "cc0c31d2e0f3494ca2d0440cf5d030d5",
      "7635b8cd4b0748e784b264301b94e535",
      "2b2393d742b344f5b1f79424dec2c4ca",
      "5f8fc5621284480bb1ca39ec6ddf7ff9",
      "13a316e0e13c492b8c9a7b7b3ffdad99",
      "9e8c49cfece042298d111c9dcd5d8831",
      "dd2b9a3f127a445995d25db6058fa425",
      "c2a59a1b016e4e4eabe47c41641d3322",
      "71d0e0924bba466f8cabc03b0a050b88",
      "ef16c90488864b088aa2f402cb3dbdf7",
      "ba44f27002c34b248ed4f70e1ac6f74d",
      "ea43a14dd52e45038a3746743ab3faac",
      "40b0e1c0bc2a40b08e4f46ab95fd5806",
      "465b13292f2b40839211874c8c7c0aee",
      "b707f3785b2d407db18f937c53d3cd65",
      "384044f6a71b46c8bcf6495c6797c74c",
      "38b584bc5a984778868f19a690a18529",
      "bd8e3a9d758140ca88d08f95fc3d0e58",
      "36b7159684c24fa18f5b250d57ef370e",
      "6431501c8c424881855cd372d2b37e77",
      "81c37bcc6d684b47841805ed43c0d8aa",
      "a710d60f9cfa405b8de2f9658c65a24f",
      "a448c57bdb41434295c745f66c0639e6",
      "6ae7d8496ac747e8989a8047775ba099",
      "0fa581a4f548419a9d7ed301a89c9712",
      "49cbb21b99d94a37b290943c926b1410",
      "c479d3b81e4e4abcae5351bb1737155d",
      "6a04740303ed4265907db573a01dd5dc",
      "90b52d08441e4366a03ce5d943d4b3c3",
      "12b42905d56947b2949ebfe2b202fe18",
      "c723559e7e9b420bbf4c417625631647",
      "49fe982496a64136a5a858a6191f1254",
      "0bb83c7a35754c46b704c68b95cf13d9",
      "053a38fae8664d7a9f509fd6f9b13b35",
      "771cd557b6ce42a586878d2d05568e89",
      "12b77bb22ca943bb9fd842278b6b135f",
      "1373256b833b402b8c2e28ab825537a5",
      "1970ae7144244e3eb650bc94f7b1c209"
     ]
    },
    "id": "d55d5f93-b22c-4a76-9261-290862c62882",
    "outputId": "ff6a42ad-fe0a-4b65-f289-e610536baa1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_env = {\"pip\": [\"sentencepiece\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff8921-cb86-49e8-9ed3-6594ecb9095a",
   "metadata": {
    "id": "d1b79e65-6a72-4046-a551-dea32116b83d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : runtime_env})\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._task = task\n",
    "        self._model = model\n",
    "        self._pipeline = None\n",
    "    \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        if (self._pipeline is None):\n",
    "            self._pipeline = pipeline(task=self._task, model=self._model)\n",
    "        outputs = self._pipeline(user_input)\n",
    "        response = outputs[0]['translation_text']\n",
    "        return response\n",
    "        \n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86d7e7-509f-4d57-9856-24a289cd1926",
   "metadata": {},
   "source": [
    "Notice how we have two different services but they are built on the same reusable code by calling `.bind()` with different initialization parameters.\n",
    "\n",
    "*We don’t need to define new deployments for every service we use.*\n",
    "\n",
    "This time we’re haven't published an application (via `serve.run()`) because these components will be invoked only by our main service deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264edf69-dad8-42c9-802c-5dce1c673063",
   "metadata": {
    "id": "1e6f7e12-cfa0-41c2-abd5-4d300b14195e"
   },
   "source": [
    "## Language detection\n",
    "\n",
    "We can create the language detection service in a similar way. \n",
    "\n",
    "> This service is lighter weight because we’re using https://github.com/pemistahl/lingua-py … which leverages traditional NLP and n-grams for detection instead of a deep learning model. It can handle more traffic than, e.g., the chat model -- and it won't require a GPU. So we can benefit from Ray Serve's fine-grained resource allocation.\n",
    "    \n",
    "Lingua is optimized for strong detection on very short text snippets, like tweets, so it should be useful for our chat exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391791e-6afe-4c57-89a7-0e50f48a0992",
   "metadata": {},
   "source": [
    "In this service implementation, we'll demonstrate the custom environment feature by requiring a pip install of lingua-language-detector wherever this deployment happens to run. Ray will ensure this environment is installed as needed. But note the `import` is deferred until the `get_response(...)` method is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636ee6a-a28f-4fa5-8a59-9fc74a8b172d",
   "metadata": {
    "id": "5f6bd63d-4958-4580-b72a-caeba1a1f578",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : {\"pip\": [\"lingua-language-detector==1.3.2\"]}})\n",
    "class LangDetect:\n",
    "    def __init__(self):\n",
    "        self._detector = None\n",
    "        \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        from lingua import Language, LanguageDetectorBuilder\n",
    "        \n",
    "        if (self._detector is None):\n",
    "            languages = [Language.ENGLISH, Language.FRENCH]\n",
    "            self._detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "        \n",
    "        output = self._detector.detect_language_of(user_input)\n",
    "        if (output == Language.ENGLISH):\n",
    "            return 'en'\n",
    "        else:\n",
    "            return 'fr'\n",
    "        \n",
    "lang_detect = LangDetect.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347781f-13ce-468f-a3f7-06aa4262d4a6",
   "metadata": {
    "id": "778ede38-d221-4f45-8e2d-71ec0702aae4",
    "tags": []
   },
   "source": [
    "## Composition patterns\n",
    "\n",
    "Let's bring the whole system together. We'll implement a service which represents our external endpoint for HTTP or Python invocations.\n",
    "* This service will have references to the deployments we've built so far, and will implement some conditional logic to ensure the correct language is used\n",
    "* Note that even if the user is interacting in French, we need to return the English response as well so that client can use that to build the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bfdad-f7d6-4f04-be2b-f64963a80084",
   "metadata": {},
   "source": [
    "### Imperative pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717ca2e-d6c6-4ef3-975d-fe34c9919a84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22f85caa-133e-4df7-8aab-4e3278ffc54b",
    "outputId": "2086ccc1-0d59-419f-d47f-d6144980f536",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Endpoint:\n",
    "    def __init__(self, chat, lang_detect, translate_en_fr, translate_fr_en):\n",
    "        # assign dependent service handles to instance variables\n",
    "        self._chat = chat\n",
    "        self._lang_detect = lang_detect\n",
    "        self._translate_en_fr = translate_en_fr\n",
    "        self._translate_fr_en = translate_fr_en\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {'response': await self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    async def get_response(self, user_input: str, history: list[str]):\n",
    "        lang_obj_ref = await self._lang_detect.get_response.remote(user_input)\n",
    "        \n",
    "        # if we didn't need the literal value of the language yet, we could pass that (future) object reference to other services\n",
    "        # here, though, we need the value in order to decide whether to call the translation services\n",
    "        # we get the Python value by awaiting the object reference\n",
    "        lang = await lang_obj_ref\n",
    "\n",
    "        if (lang == 'fr'):\n",
    "            user_input = await self._translate_fr_en.get_response.remote(user_input)\n",
    "\n",
    "        response = response_en = await self._chat.get_response.remote(user_input, history)\n",
    "        \n",
    "        if (lang == 'fr'):\n",
    "            response = await self._translate_en_fr.get_response.remote(response_en)\n",
    "            user_input = await user_input\n",
    "            \n",
    "        response = await response\n",
    "        response_en = await response_en\n",
    "        \n",
    "        return response  + '|' + user_input + '|' + response_en\n",
    "\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "endpoint = Endpoint.bind(chat, lang_detect, translate_en_fr, translate_fr_en)\n",
    "\n",
    "endpoint_handle = serve.run(endpoint, name = 'multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f16ab-1f13-4d0d-a33d-d52f6c266782",
   "metadata": {},
   "source": [
    "We've implemented control flow through our services and used the async/await pattern in several places so that we don't unnecessarily block.\n",
    "\n",
    "Then we construct the service endpoint and start a new application serving that endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63165f18-216a-411f-99f1-4fb55dd8aa8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "fbe6a39a-1b55-49c6-8a7e-632256588bf3",
    "outputId": "fd81afe6-5f31-4777-ecd5-c4d138ca901e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'My friends are cool but they eat too many carbs.'\n",
    "history = []\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35872a30-9ed0-4ac9-b11d-1cab798a5cf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b852c27-20d3-4cc3-b300-5fd15e0ba124",
    "outputId": "386cf1b6-daeb-4a19-d0cd-248131bd9f52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c90bec-1b5e-4a1a-b67a-1e11f37169ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "73855166-c226-40f9-a365-babb36f751b3",
    "outputId": "17498373-04e4-4595-80af-88cb24c568f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'Je ne suis pas sûr.'\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402694c-e4a3-4227-b438-0faab9dd9e3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25952d4a-43fe-49fa-a437-eb91813cde5c",
    "outputId": "29e847c0-4819-46c6-d15c-681bab01e028",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d25201-b26a-43c1-b8e7-0f6f5fc9fea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this point we have a service which can support the many functional and operational properties we expect to need in production, including scalability, separation of concerns, and composability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6bab4-7552-4ab5-9439-0df07d632fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48a6a1-4f60-4ca7-889a-f31866cbe4c7",
   "metadata": {
    "id": "6eae86f3-34a4-4f59-87ba-df8d9a0d3928"
   },
   "source": [
    "### Declarative pattern: Deployment Graph API\n",
    "\n",
    "What is the Deployment Graph API?\n",
    "\n",
    "* The Deployment Graph API lets us separate the flow of calls from the logic inside our services.\n",
    "\n",
    "Why might we want to use the Deployment Graph (DAG) API to separate flow from logic?\n",
    "\n",
    "* It may be valuable to add a layer of indirection – or abstraction – so that we can more easily create and compose reusable services\n",
    "* The DAG API lets us use similar patterns across the Ray platform (e.g., Ray Workflow)\n",
    "    * We can learn one general pattern for graphs and use that intuition in multiple places in our Ray applications\n",
    "* Although we compose one DAG, we retain the key Ray Serve features of granular autoscaling and resource allocation\n",
    "\n",
    "Let’s reproduce our chat service flow using the Deployment Graph API\n",
    "\n",
    "#### Getting started with deployment graphs\n",
    "\n",
    "As a first step, to keep things simple, let’s assume for a moment that we are always interacting with the service in French. \n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_simple.png' width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9def2-6de2-43a8-a465-aa3ea2478fba",
   "metadata": {
    "id": "3500503d-0831-47ae-88e0-88b8a8a7e2cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.serve.dag import InputNode\n",
    "from ray.serve.drivers import DAGDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9625b-4e2b-4d72-b314-0580096fb311",
   "metadata": {},
   "source": [
    "`InputNode` is a special type of graph node, defined by Ray Serve, which represents values supplied to our service endpoint. \n",
    "\n",
    "We can only have one `InputNode` but we can get access to multiple parameters from that node using a Python context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca019007-7f86-4b97-a0f0-60427bfeae8f",
   "metadata": {
    "id": "867cba3f-7d6e-4593-9dac-49f1069c93b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc54441-64a2-46e0-a105-c35298153bf1",
   "metadata": {
    "id": "d2463417-b8c8-41ee-a259-7708d3952ff8"
   },
   "source": [
    "Here is a minimal, linear pipeline that allows us to begin a chat in French.\n",
    "\n",
    "We build up the graph step by step, `bind`ing each deployment to its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f855710-5c45-43e0-8446-97d231cd5ac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6552ef01-a62b-497f-b578-8fc20edc6000",
    "outputId": "ad1eb72e-6c8c-473f-a92f-89d5bbebdeb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input_en = translate_fr_en.get_response.bind(user_input)    # French->English translator depends on the user input text\n",
    "chat_response = chat.get_response.bind(user_input_en, history)   # the chat deployment requires the English user input and the history\n",
    "output = translate_en_fr.get_response.bind(chat_response)        # English->French translator depends on the English chat output\n",
    "serve_dag = DAGDriver.bind(output)                               # the graph returns the output from the English->French translator\n",
    "\n",
    "handle = serve.run(serve_dag, name='basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f463b-d98a-4a55-a2ee-2c5ed0c3a2ed",
   "metadata": {},
   "source": [
    "We start the application by calling `serve.run()` on the DAGDriver, a Ray Serve component which routes HTTP requests through your call graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90e16c-9ea9-4e24-a681-7a86ce519014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "5e740b79-0e42-4883-9b38-ab3976e8f160",
    "outputId": "28adadf4-9a78-44ae-e235-9f1ddf233a75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Mes amis sont cool mais ils mangent trop de glucides.', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df0103-07f6-45c3-a643-472930f26d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed459b6-779a-43fb-948e-0eb4935f9fc3",
   "metadata": {
    "id": "d7505b58-70d8-4f50-963b-919ed251ca2e"
   },
   "source": [
    "How can we continue the chat?\n",
    "\n",
    "We need to supply English history ... but we only have French responses so far.\n",
    "\n",
    "We can use the pattern of adding a __combine node__ to our graph in order to merge the 3 elements we need to output (English chat message, English chat response, and French chat response).\n",
    "\n",
    "Combining multiple values is a common requirement -- e.g., in collecting values from a model ensemble.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/ensemble.png' width=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1066c68-7445-42fb-9065-f361cff2bd9e",
   "metadata": {
    "id": "3888b068-b6ab-4f3f-81c4-86c1dd7958e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def combine(user_input_en:str, chat_response_en: str, chat_response_fr:str)->str:\n",
    "    return chat_response_fr + '|' + user_input_en + '|' + chat_response_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707783df-c3dd-4d9b-b28f-113eb8b90625",
   "metadata": {},
   "source": [
    "The combine node here implemented here is a very simple deployment: it's built from a single function definition instead of a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840c8e5-7421-4239-bb48-c6022b8d0816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a26afc34-0c4a-49ab-b408-abba019d14a0",
    "outputId": "d5797540-1e37-406c-85b0-fdfb76c9326e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908cb48-37cc-42bd-9f31-bb365004d920",
   "metadata": {},
   "source": [
    "Event though the definitions of the `Translate` and `Chat` deployments have not changed, we call `.bind()` again to create new DAG nodes since we're composing a new DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e071816-f202-46bb-9c00-71f9d23a9b0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a26afc34-0c4a-49ab-b408-abba019d14a0",
    "outputId": "d5797540-1e37-406c-85b0-fdfb76c9326e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]\n",
    "    user_input_en = translate_fr_en.get_response.bind(user_input)\n",
    "    chat_response_en = chat.get_response.bind(user_input_en, history)\n",
    "    chat_response_fr = translate_en_fr.get_response.bind(chat_response_en)\n",
    "\n",
    "# We route the user input, the English chat response, and the French chat response into the combine node\n",
    "output = combine.bind(user_input_en, chat_response_en, chat_response_fr)\n",
    "\n",
    "# and we serve the output of the combine node\n",
    "serve_dag = DAGDriver.bind(output)\n",
    "\n",
    "handle = serve.run(serve_dag, name='enhanced_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8992b-1187-4fba-8ac1-d9c9cc0f11d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "142fcb77-1366-42a8-bd8f-30a8a960db98",
    "outputId": "cc62a275-6b55-42da-bf68-a62e04b7c784",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Mes amis sont cool mais ils mangent trop de glucides.', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16e0e6-867b-45b5-98a9-341bf7c00012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('enhanced_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ded33-d449-47cb-af31-9e9dd335453e",
   "metadata": {
    "id": "4946be16-bd7a-49a9-ae41-6b6a29643ed2"
   },
   "source": [
    "Using this pattern, we are getting everything back that we would need to offer a conversation service with the chatbot ... but only in French!\n",
    "\n",
    "### Adding Conditional Flow\n",
    "\n",
    "Our real chatbot is a bit more complex. It has a conditional flow where we invoke the translation service only when the user is *not* interacting in English.\n",
    "\n",
    "We can add the remaining elements of our service and the basic API changes will be fairly minimal. But there is one aspect that requires us to do a little bit of thinking and employ a new pattern.\n",
    "\n",
    "#### Static Graphs and Conditional Control Flow\n",
    "\n",
    "The graph we define with the DAG API is static – it’s created ahead of time. \n",
    "\n",
    "In the first DAG demo, we were always invoking the same sequence of services, so the static character of the graph might not have been obvious… but now we’re focusing on it so you can see where things might get a bit more complicated.\n",
    "\n",
    "To implement branching flow control with the DAG API, we’ll use a special pattern so that the same graph always runs … but certain nodes (in our case, translator nodes) behave differently based on data they receive.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_complex.png' width=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2ccd6-e926-47ba-9f49-7407a7528f41",
   "metadata": {
    "id": "0e793082-96ce-447c-9af7-1be949be9b21",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : runtime_env})\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._task = task\n",
    "        self._model = model\n",
    "        self._pipeline = None\n",
    "    \n",
    "    def get_response(self, user_input:str, user_lang:str) -> str:\n",
    "        if (user_lang == 'en'):\n",
    "            return user_input # no-op\n",
    "        \n",
    "        if (self._pipeline is None):\n",
    "            self._pipeline = pipeline(task=self._task, model=self._model)\n",
    "            \n",
    "        outputs = self._pipeline(user_input)\n",
    "        response = outputs[0]['translation_text']\n",
    "        return response        \n",
    "        \n",
    "\n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c9dc2-bea3-43da-a432-cdb95a16eba3",
   "metadata": {},
   "source": [
    "The if-else control flow inside `get_response()` calls the transation logic only when the user is *not* using English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe9ca4-023b-44aa-9294-de71ceed92db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0105c9c-58e2-48c7-a4ff-e5d1a26632e6",
    "outputId": "2aeb1446-0e3e-463f-95e5-24fa9336f9cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_detect = LangDetect.bind()\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]\n",
    "    user_lang = lang_detect.get_response.bind(user_input)\n",
    "    user_input_en = translate_fr_en.get_response.bind(user_input, user_lang)\n",
    "    chat_response_en = chat.get_response.bind(user_input_en, history)\n",
    "    chat_response_fr = translate_en_fr.get_response.bind(chat_response_en, user_lang)\n",
    "    output = combine.bind(user_input_en, chat_response_en, chat_response_fr)\n",
    "    serve_dag = DAGDriver.bind(output)\n",
    "\n",
    "handle = serve.run(serve_dag, name='full_chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94dd03-9027-46d6-9282-58e16946add6",
   "metadata": {},
   "source": [
    "In this code, the translation services are always part of the graph and participate in the data flow. So the graph is static, even though the translation behavior is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1755dc-1555-42ee-8283-76b37a6c9fcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "5bf055b3-2c97-435d-ab36-d540b3fef963",
    "outputId": "1a7da7c7-fb2b-4bde-cd58-8142c430b83b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'Mes amis sont cool mais ils mangent trop de glucides.'\n",
    "history = []\n",
    "\n",
    "response = ray.get(handle.predict.remote(message, history))\n",
    "\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e872e-dd8e-4907-a8fd-a67bf10dc554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930c43a-fb96-4092-ad6c-10f01a3f7b10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "d7cb4815-093b-49bd-b6d4-98fdb1f9c038",
    "outputId": "ae2a3adc-dcc0-4613-bcbe-3945a415cfee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Truly bread is delightful', history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a48e72-edd8-49bf-b41e-787b10b80039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('full_chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb75ac-aa59-4d69-9740-a44e61d13e08",
   "metadata": {},
   "source": [
    "## Architecture / under-the-hood\n",
    "\n",
    "### Ray cluster perspective: actors\n",
    "\n",
    "In Ray, user code is executed by worker processes. These workers can run tasks (stateless functions) or actors (stateful class instances).\n",
    "\n",
    "Ray Serve is built on actors, allowing deployments to collect expensive state once (such as loading a ML model) and to reuse it across many service requests.\n",
    "\n",
    "Although you may never need to code any Ray tasks or actors yourself, your Ray Serve application has full access to those cluster capabilities and you may wish to use them to implement other functionality (e.g., service or operations that don't need to accept HTTP traffic). More information is at https://docs.ray.io/en/latest/ray-core/walkthrough.html\n",
    "\n",
    "### Serve design\n",
    "\n",
    "Under the hood, a few other actors are used to make up a serve instance.\n",
    "\n",
    "* Controller: A global actor unique to each Serve instance is responsible for managing other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "* HTTP Proxy: By default there is one HTTP proxy actor on the head node that accepts incoming requests, forwards them to replicas, and responds once they are completed. For scalability and high availability, you can also run a proxy on each node in the cluster via the location field of http_options.\n",
    "\n",
    "* Deployment Replicas: Actors that execute the code in response to a request. Each replica processes requests from the HTTP proxy.\n",
    "<img src='https://docs.ray.io/en/latest/_images/architecture-2.0.svg' width=700 />\n",
    "\n",
    "Incoming requests, once resolved to a particular deployment, are queued. The requests from the queue are assigned round-robin to available replicas as long as capacity is available. This design provides load balancing and elasticity. \n",
    "\n",
    "Capacity can be managed with the `max_concurrent_queries` parameter to the deployment decorator. This value defaults to 100 and represents the maximum number of queries that will be sent to a replica of this deployment without receiving a response. Each replica has its own queue to collect and smooth incoming request traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e217e9-ba60-4172-858b-65fd5fb03359",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling and performance\n",
    "\n",
    "### Replicas and autoscaling\n",
    "\n",
    "Each deployment can have its own resource management and autoscaling configuration, with several options for scaling.\n",
    "\n",
    "By default -- if nothing specified, as in our examples above -- the default is a single. We can specify a larger, constant number of replicas in the decorator:\n",
    "```python\n",
    "@serve.deployment(num_replicas=3)\n",
    "```\n",
    "\n",
    "For autoscaling, instead of `num_replicas`, we provide an `autoscaling_config` dictionary. With autoscaling, we can specify a minimum and maximum range for the number of replicas, the initial replica count, a load target, and more.\n",
    "\n",
    "Here is example of extended configuration -- see https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation for more details:\n",
    "\n",
    "```python\n",
    "@serve.deployment(\n",
    "    autoscaling_config={\n",
    "        'min_replicas': 1,\n",
    "        'initial_replicas': 2,\n",
    "        'max_replicas': 5,\n",
    "        'target_num_ongoing_requests_per_replica': 10,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "`min_replicas` can also be set to zero to create a \"serverless\" style design: in exchange for potentially slower startup, no actors (or their CPU/GPU resources) need to be permanently reserved.\n",
    "\n",
    "### Autoscaling LLM chat\n",
    "\n",
    "The LLM-baset chat service is a good example for seeing autoscaling in action, because LLM inference is relative expensive so we can easily build up a queue of requests to the service. The autoscaler responds to the dynamics of queue sizes and will launch additional replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101cbf7-8ac2-46fa-8684-682666f566cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbde82-e3ab-486e-a0e3-1112d31e6fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5}, autoscaling_config={ 'min_replicas': 1, 'max_replicas': 4 })\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        history.append(user_input)\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response\n",
    "    \n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "handle = serve.run(chat, name='autoscale_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700ec18-fbcb-4db2-98cb-5d6a8ff9f244",
   "metadata": {},
   "source": [
    "We can generate a little load and look at the Ray Dashboard\n",
    "\n",
    "What do we expect to see?\n",
    "\n",
    "* Autoscaling of the Chat service up to 4 replicas\n",
    "* Efficient use of fractional GPU resources\n",
    "    * If our cluster has just 2 GPUs, we can run 4 replicase there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000403d0-8073-418d-9e57-8177025d20ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_request(s):\n",
    "    return requests.post(\"http://localhost:8000/\", json = s).json()\n",
    "\n",
    "sample = '{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'\n",
    "make_request(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d0eba-b03d-4082-b301-16d29bbfe2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "results = executor.map(make_request, ['{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'] * 40)\n",
    "\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954e172-1e1e-4c73-9d65-2886a8c3271c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('autoscale_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cae4aa-7c84-40fa-8f40-1cbd8bd64445",
   "metadata": {},
   "source": [
    "### Request batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557d733-aa66-4f12-9b01-8fefdd1a4128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment()\n",
    "class Chat:\n",
    "    def __init__(self):\n",
    "        self._message = \"Chatbot counts the batch size at \"\n",
    "\n",
    "    @serve.batch(max_batch_size=5)\n",
    "    async def handle_batch(self, request_batch):\n",
    "        num_requests = len(request_batch)\n",
    "        return [ {'response': self._message + str(num_requests) } ] * num_requests\n",
    "    \n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return await self.handle_batch(data)\n",
    "    \n",
    "chat = Chat.bind()\n",
    "\n",
    "handle = serve.run(chat, name='batch_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808017b-4aee-4bc4-b188-2e6311cca439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = executor.map(make_request, ['{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576e111-35da-48c2-84c4-4bea3544fb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d56dd1-46b7-45da-9a33-10deb32c20fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('batch_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc07e1-4574-419d-adb2-ca593d471963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
