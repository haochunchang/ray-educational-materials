{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399be64-44d3-45e4-9b43-3757a9b92daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca49cb-b3f5-45f3-8600-02c6a8b8069c",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Outline\n",
    "\n",
    "-   Deployments\n",
    "    -   Resources (CPU/GPU/custom)\n",
    "    -   Runtime environments support, usage (functionality)\n",
    "    -   Bound deployments, ServeHandles\n",
    "-   Composition Patterns\n",
    "    -   Imperative\n",
    "    -   Declarative / Graph Deployment API\n",
    "-   Architecture / Under-the-hood\n",
    "    -   Ray cluster perspective - processes / workers / actors\n",
    "    -   Request routing, queuing, load balancing in Serve\n",
    "-   Scaling and Performance\n",
    "    -   Replicas\n",
    "        -   num_replicas, autoscaling_config, max_concurrent_queries\n",
    "    -   Request batching (+ async)\n",
    "\n",
    "### Example scenario: multilingual LLM chat\n",
    "\n",
    "For our example use case, we’ll see how to leverage Ray Serve to host a LLM Chat\n",
    "model and how to enhance it using additional services for multilingual\n",
    "interactions.\n",
    "\n",
    "### Context: Ray AIR\n",
    "\n",
    "Ray AIR is the Ray AI Runtime, a set of high-level easy-to-use APIs for\n",
    "ingesting data, training models – including reinforcement learning\n",
    "models – tuning those models and then serving them.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=600 loading=\"lazy\"/>\n",
    "\n",
    "Key principles behind Ray and Ray AIR are\n",
    "* Performance\n",
    "* Developer experience and simplicity\n",
    "\n",
    "__below diagram to be replaced shortly__\n",
    "\n",
    "<img src='https://docs.ray.io/en/latest/_images/why-air.svg' width=600 />\n",
    "\n",
    "## Ray Serve\n",
    "\n",
    "Serve is a microservices framework for serving ML – the model serving\n",
    "component of Ray AIR.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "## Deployments\n",
    "\n",
    "`Deployment` is the fundamental user-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "### Our First Service\n",
    "\n",
    "Let’s jump right in and get something simple up and running on Ray\n",
    "Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa8d0-4d4a-41bd-b206-9de536da5c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Chat:\n",
    "    def __init__(self, msg: str):\n",
    "        self._msg = msg # initial state\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {\"result\": self.get_response(data['input']) }\n",
    "    \n",
    "    def get_response(self, message: str) -> str:\n",
    "        return self._msg + message\n",
    "\n",
    "handle = serve.run(Chat.bind(msg=\"Yes... \"), name='helloworld')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed55cd-4d79-40a6-8737-09ac0fc33782",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint or by invoking directly from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffe466-169f-4d73-9e3c-019d8c62fc2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_json = '{ \"input\" : \"hello\" }'\n",
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40134f3-a5ba-4554-95f9-74a7f7731e66",
   "metadata": {},
   "source": [
    "#### << Deployment, Bound Deployment, ServeHandle at the usage level; internals for servehandle come later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50986b5-b5d5-48e1-b7ce-d5fa27a2121e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(handle))\n",
    "print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b390c1-38fb-40fc-b81f-8b5fcd2c378c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = handle.get_response.remote('hello')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca7ca-3c5d-4cb8-9e8e-88d98b2a1640",
   "metadata": {},
   "source": [
    "#### << mini explanation around remote method call, ObjectRef as future/promise>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142e154-6313-4eee-acf9-ea7e640ed78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346a23e-b29f-4f0a-847a-989e00640f41",
   "metadata": {},
   "source": [
    "Since we'll be creating a new application example, we can delete the old one -- that allows Ray to remove the replicas of our Chat deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c62ae7-8739-45cf-b839-f3a22b4463b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete('helloworld')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a4917-c20b-4da2-b27c-0517b99e27ce",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f88fe-73db-41b7-a0d0-df0e454c21d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5})\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        # configure stateful elements of our service such as loading a model\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        # path to handle HTTP requests\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        # after decoding the payload, we delegate to get_response for logic\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        # this method receives calls directly (from Python) or from __call__ (from HTTP)\n",
    "        history.append(user_input)\n",
    "        # the history is client-side state and will be a list of raw strings;\n",
    "        # for the default config of the model and tokenizer, history should be joined with '</s><s>'\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae819eab-26bf-409c-92de-7f6a8c79cf65",
   "metadata": {
    "tags": []
   },
   "source": [
    "<<  ray_actor_options, cpu/gpu, custom resources\n",
    "\n",
    "<< best practice resource vs node ~ cattle vs pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a4535-fdf4-4bb1-858f-203a20eb7afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "handle = serve.run(chat, name='basic_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f2cad-1123-4607-8f02-8873ec2609d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'My friends are cool but they eat too many carbs.'\n",
    "history = []\n",
    "response_handle = handle.get_response.remote(message, history)\n",
    "response = ray.get(response_handle)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca11a1-e4d7-450e-844a-367504a98f05",
   "metadata": {},
   "source": [
    "We prepare a message and a chat history list and call our chat service via Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5b70e-df82-4f9d-84b5-3e27887e87b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34549288-e4a2-4fbb-b41f-e7ea70f51d13",
    "outputId": "8c682533-9ebd-4965-dc75-b0661cf1f3d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += [message, response]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959662cb-cdd0-4d02-9fbb-572c8735b996",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "e1b448ae-3662-4211-bde6-72734f80ad30",
    "outputId": "28f4ea8b-2ce7-4aee-d1df-5546a74a5f9b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = \"I'm not sure.\"\n",
    "response_handle = handle.get_response.remote(message, history)\n",
    "response = ray.get(response_handle)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818e8bd-cdba-4fe5-84b4-3c62f2a2358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete('basic_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa14cd8-769b-43c5-b7d5-d294ed4e66ba",
   "metadata": {},
   "source": [
    "### Runtime environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891148a-7900-4ac9-9d4e-7d943955c10c",
   "metadata": {},
   "source": [
    "Patterns for managing dependencies\n",
    "- generally available (cluster env, container, etc)\n",
    "- per-task/actor/job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb611ab-85d7-40c6-b1fd-45a3c4e2144e",
   "metadata": {
    "id": "b54a5b65-7b86-4db0-821c-f3f9ce143316",
    "tags": []
   },
   "source": [
    "#### Composing services with Ray for chatbot en Français: roadmap for additional Services\n",
    "\n",
    "The underlying chatbot model we’ve used only supports English interaction. But we can use the following recipe to add French language support:\n",
    "\n",
    "1. Implement a translation service between French and English\n",
    "1. Implement a language detection service\n",
    "1. Implement a routing (dispatch) service:\n",
    "    1. If the incoming prompt is French, then\n",
    "        1. Route the inbound prompt through the FR-EN translator\n",
    "        1. Pass the EN prompt to the chat model\n",
    "        1. Pass the EN output from the chat model through the EN-FR translator\n",
    "        1. Return the French response\n",
    "    1. Otherwise (if the prompt is in English), pass it straight to the chatbot as we did earlier and return the (English) response\n",
    "\n",
    "Let’s look using Ray Serve to implement model inference with these composed and conditional-flow elements using Python method calls (https://docs.ray.io/en/latest/serve/key-concepts.html#servehandle-composing-deployments). Later we’ll look at an alternative approach using Ray’s Deployment Graph API.\n",
    "\n",
    "We’ll implement parts 1 and 2 first…\n",
    "\n",
    "#### Translation service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c808e-62ec-4d7b-ad57-fb31d1ac32b9",
   "metadata": {},
   "source": [
    "contrast 2 patterns: generally available lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b992cb-3752-4cf9-b6d7-18ccb7b69300",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358,
     "referenced_widgets": [
      "ed182dd854d1439ab6d44e39210d7074",
      "4ed3f579be5d450aacfa0f970b8d81da",
      "5f0dc2de13284933ae2439af1fe88753",
      "3d98bfe346ed40a8bd5d46de450bca24",
      "1737ac3e202a41688c4adb0b498ec58f",
      "3f127b730e024ffa91ab921ac5291c73",
      "bbd6c25a5bef464fa74837811f1e3a39",
      "442b5bbd50f145e9844495dcda8c09d5",
      "13efed5b0c204ce58120470ae37e6af2",
      "5f28f3c322484ff5a88e988790417d28",
      "d5ff21c53b644abbaa8c2a06c38d2d1d",
      "9860b04b1772493e819f80ceb1000aa0",
      "506f934b75c84618a49f92188d1520b3",
      "4de12f5510974cd79848308c361db070",
      "5b28b54f06f7445aaeab92194bc5951f",
      "eb16d41e87a14b18a33ce16aab9c9c8c",
      "ecd2111b9e974b3eaf712610ac19bb9c",
      "cc0c31d2e0f3494ca2d0440cf5d030d5",
      "7635b8cd4b0748e784b264301b94e535",
      "2b2393d742b344f5b1f79424dec2c4ca",
      "5f8fc5621284480bb1ca39ec6ddf7ff9",
      "13a316e0e13c492b8c9a7b7b3ffdad99",
      "9e8c49cfece042298d111c9dcd5d8831",
      "dd2b9a3f127a445995d25db6058fa425",
      "c2a59a1b016e4e4eabe47c41641d3322",
      "71d0e0924bba466f8cabc03b0a050b88",
      "ef16c90488864b088aa2f402cb3dbdf7",
      "ba44f27002c34b248ed4f70e1ac6f74d",
      "ea43a14dd52e45038a3746743ab3faac",
      "40b0e1c0bc2a40b08e4f46ab95fd5806",
      "465b13292f2b40839211874c8c7c0aee",
      "b707f3785b2d407db18f937c53d3cd65",
      "384044f6a71b46c8bcf6495c6797c74c",
      "38b584bc5a984778868f19a690a18529",
      "bd8e3a9d758140ca88d08f95fc3d0e58",
      "36b7159684c24fa18f5b250d57ef370e",
      "6431501c8c424881855cd372d2b37e77",
      "81c37bcc6d684b47841805ed43c0d8aa",
      "a710d60f9cfa405b8de2f9658c65a24f",
      "a448c57bdb41434295c745f66c0639e6",
      "6ae7d8496ac747e8989a8047775ba099",
      "0fa581a4f548419a9d7ed301a89c9712",
      "49cbb21b99d94a37b290943c926b1410",
      "c479d3b81e4e4abcae5351bb1737155d",
      "6a04740303ed4265907db573a01dd5dc",
      "90b52d08441e4366a03ce5d943d4b3c3",
      "12b42905d56947b2949ebfe2b202fe18",
      "c723559e7e9b420bbf4c417625631647",
      "49fe982496a64136a5a858a6191f1254",
      "0bb83c7a35754c46b704c68b95cf13d9",
      "053a38fae8664d7a9f509fd6f9b13b35",
      "771cd557b6ce42a586878d2d05568e89",
      "12b77bb22ca943bb9fd842278b6b135f",
      "1373256b833b402b8c2e28ab825537a5",
      "1970ae7144244e3eb650bc94f7b1c209"
     ]
    },
    "id": "d55d5f93-b22c-4a76-9261-290862c62882",
    "outputId": "ff6a42ad-fe0a-4b65-f289-e610536baa1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff8921-cb86-49e8-9ed3-6594ecb9095a",
   "metadata": {
    "id": "d1b79e65-6a72-4046-a551-dea32116b83d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._pipeline = pipeline(task=task, model=model)\n",
    "    \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        outputs = self._pipeline(user_input)\n",
    "        response = outputs[0]['translation_text']\n",
    "        return response\n",
    "        \n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86d7e7-509f-4d57-9856-24a289cd1926",
   "metadata": {},
   "source": [
    "Notice how we have two different services but they are built on the same reusable code by calling `.bind()` with different initialization parameters.\n",
    "\n",
    "*We don’t need to define new deployments for every service we use.*\n",
    "\n",
    "This time we’re haven't published an application (via `serve.run()`) because these components will be invoked only by our main service deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264edf69-dad8-42c9-802c-5dce1c673063",
   "metadata": {
    "id": "1e6f7e12-cfa0-41c2-abd5-4d300b14195e"
   },
   "source": [
    "### Language detection\n",
    "\n",
    "We can create the language detection service in a similar way. \n",
    "\n",
    "> This service is lighter weight because we’re using https://github.com/pemistahl/lingua-py … which leverages traditional NLP and n-grams for detection instead of a deep learning model. It can handle more traffic than, e.g., the chat model -- and it won't require a GPU. So we can benefit from Ray Serve's fine-grained resource allocation.\n",
    "    \n",
    "Lingua is optimized for strong detection on very short text snippets, like tweets, so it should be useful for our chat exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391791e-6afe-4c57-89a7-0e50f48a0992",
   "metadata": {},
   "source": [
    "Alternate pattern: environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5994733-195d-439f-99e7-a17cca651f36",
   "metadata": {
    "id": "8d365afa-f56d-4d11-9b0c-ceb58d206f0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_env = {\"pip\": [\"lingua-language-detector\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636ee6a-a28f-4fa5-8a59-9fc74a8b172d",
   "metadata": {
    "id": "5f6bd63d-4958-4580-b72a-caeba1a1f578",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : runtime_env})\n",
    "class LangDetect:\n",
    "    from lingua import Language, LanguageDetectorBuilder\n",
    "                  \n",
    "    def __init__(self):\n",
    "        languages = [Language.ENGLISH, Language.FRENCH]\n",
    "        self._detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "    \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        output = self._detector.detect_language_of(user_input)\n",
    "        if (output == Language.ENGLISH):\n",
    "            return 'en'\n",
    "        else:\n",
    "            return 'fr'\n",
    "        \n",
    "lang_detect = LangDetect.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef08789-9ebc-440d-bd2d-1f5b097cefbd",
   "metadata": {},
   "source": [
    "#### env options, pros/cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347781f-13ce-468f-a3f7-06aa4262d4a6",
   "metadata": {
    "id": "778ede38-d221-4f45-8e2d-71ec0702aae4",
    "tags": []
   },
   "source": [
    "## Composition patterns\n",
    "\n",
    "Let's bring the whole system together. We'll implement a service which represents our external endpoint for HTTP or Python invocations.\n",
    "* This service will have references to the deployments we've built so far, and will implement some conditional logic to ensure the correct language is used\n",
    "* Note that even if the user is interacting in French, we need to return the English response as well so that client can use that to build the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bfdad-f7d6-4f04-be2b-f64963a80084",
   "metadata": {},
   "source": [
    "### Imperative pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717ca2e-d6c6-4ef3-975d-fe34c9919a84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22f85caa-133e-4df7-8aab-4e3278ffc54b",
    "outputId": "2086ccc1-0d59-419f-d47f-d6144980f536",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Endpoint:\n",
    "    def __init__(self, chat, lang_detect, translate_en_fr, translate_fr_en):\n",
    "        # assign dependent service handles to instance variables\n",
    "        self._chat = chat\n",
    "        self._lang_detect = lang_detect\n",
    "        self._translate_en_fr = translate_en_fr\n",
    "        self._translate_fr_en = translate_fr_en\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {'response': await self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    async def get_response(self, user_input: str, history: list[str]):\n",
    "        lang_obj_ref = await self._lang_detect.get_response.remote(user_input)\n",
    "        \n",
    "        # if we didn't need the literal value of the language yet, we could pass that (future) object reference to other services\n",
    "        # here, though, we need the value in order to decide whether to call the translation services\n",
    "        # we get the Python value by awaiting the object reference\n",
    "        lang = await lang_obj_ref\n",
    "\n",
    "        if (lang == 'fr'):\n",
    "            user_input = await self._translate_fr_en.get_response.remote(user_input)\n",
    "\n",
    "        response = response_en = await self._chat.get_response.remote(user_input, history)\n",
    "        \n",
    "        if (lang == 'fr'):\n",
    "            response = await self._translate_en_fr.get_response.remote(response_en)\n",
    "            user_input = await user_input\n",
    "            \n",
    "        response = await response\n",
    "        response_en = await response_en\n",
    "        \n",
    "        return response  + '|' + user_input + '|' + response_en\n",
    "\n",
    "endpoint = Endpoint.bind(chat, lang_detect, translate_en_fr, translate_fr_en)\n",
    "\n",
    "endpoint_handle = serve.run(endpoint, name = 'multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f16ab-1f13-4d0d-a33d-d52f6c266782",
   "metadata": {},
   "source": [
    "We've implemented control flow through our services and used the async/await pattern in several places so that we don't unnecessarily block.\n",
    "\n",
    "Then we construct the service endpoint and start a new application serving that endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63165f18-216a-411f-99f1-4fb55dd8aa8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "fbe6a39a-1b55-49c6-8a7e-632256588bf3",
    "outputId": "fd81afe6-5f31-4777-ecd5-c4d138ca901e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'My friends are cool but they eat too many carbs.'\n",
    "history = []\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35872a30-9ed0-4ac9-b11d-1cab798a5cf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b852c27-20d3-4cc3-b300-5fd15e0ba124",
    "outputId": "386cf1b6-daeb-4a19-d0cd-248131bd9f52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c90bec-1b5e-4a1a-b67a-1e11f37169ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "73855166-c226-40f9-a365-babb36f751b3",
    "outputId": "17498373-04e4-4595-80af-88cb24c568f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'Je ne suis pas sûr.'\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402694c-e4a3-4227-b438-0faab9dd9e3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25952d4a-43fe-49fa-a437-eb91813cde5c",
    "outputId": "29e847c0-4819-46c6-d15c-681bab01e028",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d25201-b26a-43c1-b8e7-0f6f5fc9fea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this point we have a service which can support the many functional and operational properties we expect to need in production, including scalability, separation of concerns, and composability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6bab4-7552-4ab5-9439-0df07d632fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48a6a1-4f60-4ca7-889a-f31866cbe4c7",
   "metadata": {
    "id": "6eae86f3-34a4-4f59-87ba-df8d9a0d3928"
   },
   "source": [
    "### Deployment Graph API\n",
    "\n",
    "What is the Deployment Graph API?\n",
    "\n",
    "* The Deployment Graph API lets us separate the flow of calls from the logic inside our services.\n",
    "\n",
    "Why might we want to use the Deployment Graph (DAG) API to separate flow from logic?\n",
    "\n",
    "* It may be valuable to add a layer of indirection – or abstraction – so that we can more easily create and compose reusable services\n",
    "* The DAG API lets us use similar patterns across the Ray platform (e.g., Ray Workflow)\n",
    "    * We can learn one general pattern for graphs and use that intuition in multiple places in our Ray applications\n",
    "* Although we compose one DAG, we retain the key Ray Serve features of granular autoscaling and resource allocation\n",
    "\n",
    "Let’s reproduce our chat service flow using the Deployment Graph API\n",
    "\n",
    "#### Getting Started with Deployment Graphs\n",
    "\n",
    "As a first step, to keep things simple, let’s assume for a moment that we are always interacting with the service in French. \n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_simple.png' width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9def2-6de2-43a8-a465-aa3ea2478fba",
   "metadata": {
    "id": "3500503d-0831-47ae-88e0-88b8a8a7e2cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.serve.dag import InputNode\n",
    "from ray.serve.drivers import DAGDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9625b-4e2b-4d72-b314-0580096fb311",
   "metadata": {},
   "source": [
    "`InputNode` is a special type of graph node, defined by Ray Serve, which represents values supplied to our service endpoint. \n",
    "\n",
    "We can only have one `InputNode` but we can get access to multiple parameters from that node using a Python context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca019007-7f86-4b97-a0f0-60427bfeae8f",
   "metadata": {
    "id": "867cba3f-7d6e-4593-9dac-49f1069c93b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc54441-64a2-46e0-a105-c35298153bf1",
   "metadata": {
    "id": "d2463417-b8c8-41ee-a259-7708d3952ff8"
   },
   "source": [
    "Here is a minimal, linear pipeline that allows us to begin a chat in French.\n",
    "\n",
    "We build up the graph step by step, `bind`ing each deployment to its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f855710-5c45-43e0-8446-97d231cd5ac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6552ef01-a62b-497f-b578-8fc20edc6000",
    "outputId": "ad1eb72e-6c8c-473f-a92f-89d5bbebdeb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input_en = translate_fr_en.get_response.bind(user_input)    # French->English translator depends on the user input text\n",
    "chat_response = chat.get_response.bind(user_input_en, history)   # the chat deployment requires the English user input and the history\n",
    "output = translate_en_fr.get_response.bind(chat_response)        # English->French translator depends on the English chat output\n",
    "serve_dag = DAGDriver.bind(output)                               # the graph returns the output from the English->French translator\n",
    "\n",
    "handle = serve.run(serve_dag, name='basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f463b-d98a-4a55-a2ee-2c5ed0c3a2ed",
   "metadata": {},
   "source": [
    "We start the application by calling `serve.run()` on the DAGDriver, a Ray Serve component which routes HTTP requests through your call graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90e16c-9ea9-4e24-a681-7a86ce519014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "5e740b79-0e42-4883-9b38-ab3976e8f160",
    "outputId": "28adadf4-9a78-44ae-e235-9f1ddf233a75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Mes amis sont cool mais ils mangent trop de glucides.', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df0103-07f6-45c3-a643-472930f26d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete('basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed459b6-779a-43fb-948e-0eb4935f9fc3",
   "metadata": {
    "id": "d7505b58-70d8-4f50-963b-919ed251ca2e"
   },
   "source": [
    "How can we continue the chat?\n",
    "\n",
    "We need to supply English history ... but we only have French responses so far.\n",
    "\n",
    "We can use the pattern of adding a __combine node__ to our graph in order to merge the 3 elements we need to output (English chat message, English chat response, and French chat response).\n",
    "\n",
    "Combining multiple values is a common requirement -- e.g., in collecting values from a model ensemble.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/ensemble.png' width=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1066c68-7445-42fb-9065-f361cff2bd9e",
   "metadata": {
    "id": "3888b068-b6ab-4f3f-81c4-86c1dd7958e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def combine(user_input_en:str, chat_response_en: str, chat_response_fr:str)->str:\n",
    "    return chat_response_fr + '|' + user_input_en + '|' + chat_response_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707783df-c3dd-4d9b-b28f-113eb8b90625",
   "metadata": {},
   "source": [
    "The combine node here implemented here is a very simple deployment: it's built from a single function definition instead of a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840c8e5-7421-4239-bb48-c6022b8d0816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a26afc34-0c4a-49ab-b408-abba019d14a0",
    "outputId": "d5797540-1e37-406c-85b0-fdfb76c9326e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908cb48-37cc-42bd-9f31-bb365004d920",
   "metadata": {},
   "source": [
    "Event though the definitions of the `Translate` and `Chat` deployments have not changed, we call `.bind()` again to create new DAG nodes since we're composing a new DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e071816-f202-46bb-9c00-71f9d23a9b0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a26afc34-0c4a-49ab-b408-abba019d14a0",
    "outputId": "d5797540-1e37-406c-85b0-fdfb76c9326e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]\n",
    "    user_input_en = translate_fr_en.get_response.bind(user_input)\n",
    "    chat_response_en = chat.get_response.bind(user_input_en, history)\n",
    "    chat_response_fr = translate_en_fr.get_response.bind(chat_response_en)\n",
    "\n",
    "# We route the user input, the English chat response, and the French chat response into the combine node\n",
    "output = combine.bind(user_input_en, chat_response_en, chat_response_fr)\n",
    "\n",
    "# and we serve the output of the combine node\n",
    "serve_dag = DAGDriver.bind(output)\n",
    "\n",
    "handle = serve.run(serve_dag, name='enhanced_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8992b-1187-4fba-8ac1-d9c9cc0f11d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "142fcb77-1366-42a8-bd8f-30a8a960db98",
    "outputId": "cc62a275-6b55-42da-bf68-a62e04b7c784",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Mes amis sont cool mais ils mangent trop de glucides.', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16e0e6-867b-45b5-98a9-341bf7c00012",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete('enhanced_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ded33-d449-47cb-af31-9e9dd335453e",
   "metadata": {
    "id": "4946be16-bd7a-49a9-ae41-6b6a29643ed2"
   },
   "source": [
    "Using this pattern, we are getting everything back that we would need to offer a conversation service with the chatbot ... but only in French!\n",
    "\n",
    "### Adding Conditional Flow\n",
    "\n",
    "Our real chatbot is a bit more complex. It has a conditional flow where we invoke the translation service only when the user is *not* interacting in English.\n",
    "\n",
    "We can add the remaining elements of our service and the basic API changes will be fairly minimal. But there is one aspect that requires us to do a little bit of thinking and employ a new pattern.\n",
    "\n",
    "#### Static Graphs and Conditional Control Flow\n",
    "\n",
    "The graph we define with the DAG API is static – it’s created ahead of time. \n",
    "\n",
    "In the first DAG demo, we were always invoking the same sequence of services, so the static character of the graph might not have been obvious… but now we’re focusing on it so you can see where things might get a bit more complicated.\n",
    "\n",
    "To implement branching flow control with the DAG API, we’ll use a special pattern so that the same graph always runs … but certain nodes (in our case, translator nodes) behave differently based on data they receive.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_complex.png' width=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2ccd6-e926-47ba-9f49-7407a7528f41",
   "metadata": {
    "id": "0e793082-96ce-447c-9af7-1be949be9b21",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._pipeline = pipeline(task=task, model=model)\n",
    "    \n",
    "    def get_response(self, user_input:str, user_lang:str) -> str:\n",
    "        if (user_lang == 'en'):\n",
    "            return user_input # no-op\n",
    "        else:\n",
    "            outputs = self._pipeline(user_input)\n",
    "            response = outputs[0]['translation_text']\n",
    "            return response\n",
    "        \n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c9dc2-bea3-43da-a432-cdb95a16eba3",
   "metadata": {},
   "source": [
    "The if-else control flow inside `get_response()` calls the transation logic only when the user is *not* using English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe9ca4-023b-44aa-9294-de71ceed92db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0105c9c-58e2-48c7-a4ff-e5d1a26632e6",
    "outputId": "2aeb1446-0e3e-463f-95e5-24fa9336f9cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_detect = LangDetect.bind()\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]\n",
    "    user_lang = lang_detect.get_response.bind(user_input)\n",
    "    user_input_en = translate_fr_en.get_response.bind(user_input, user_lang)\n",
    "    chat_response_en = chat.get_response.bind(user_input_en, history)\n",
    "    chat_response_fr = translate_en_fr.get_response.bind(chat_response_en, user_lang)\n",
    "    output = combine.bind(user_input_en, chat_response_en, chat_response_fr)\n",
    "    serve_dag = DAGDriver.bind(output)\n",
    "\n",
    "handle = serve.run(serve_dag, name='full_chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94dd03-9027-46d6-9282-58e16946add6",
   "metadata": {},
   "source": [
    "In this code, the translation services are always part of the graph and participate in the data flow. So the graph is static, even though the translation behavior is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1755dc-1555-42ee-8283-76b37a6c9fcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "5bf055b3-2c97-435d-ab36-d540b3fef963",
    "outputId": "1a7da7c7-fb2b-4bde-cd58-8142c430b83b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'Mes amis sont cool mais ils mangent trop de glucides.'\n",
    "history = []\n",
    "\n",
    "response = ray.get(handle.predict.remote(message, history))\n",
    "\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e872e-dd8e-4907-a8fd-a67bf10dc554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930c43a-fb96-4092-ad6c-10f01a3f7b10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "d7cb4815-093b-49bd-b6d4-98fdb1f9c038",
    "outputId": "ae2a3adc-dcc0-4613-bcbe-3945a415cfee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Truly bread is delightful', history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a48e72-edd8-49bf-b41e-787b10b80039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('full_chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb75ac-aa59-4d69-9740-a44e61d13e08",
   "metadata": {},
   "source": [
    "## Architecture / under-the-hood\n",
    "\n",
    "### Ray cluster perspective\n",
    "\n",
    "Worker processes, tasks, actors\n",
    "\n",
    "### Serve design\n",
    "\n",
    "Request routing, queueing, load balancing\n",
    "\n",
    "servehandles impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e217e9-ba60-4172-858b-65fd5fb03359",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling and performance\n",
    "\n",
    "### Replicas\n",
    "\n",
    "* Each deployment can have its own resource management and autoscaling configuration\n",
    "\n",
    "Here is example of extended configuration -- see https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation for more details\n",
    "\n",
    "Default is 1 replica\n",
    "\n",
    "fixes replicas\n",
    "\n",
    "```python\n",
    "@serve.deployment(num_replicas=3)\n",
    "```\n",
    "\n",
    "autoscale\n",
    "\n",
    "```python\n",
    "@serve.deployment(\n",
    "    autoscaling_config={\n",
    "        'min_replicas': 1,\n",
    "        'initial_replicas': 2,\n",
    "        'max_replicas': 5,\n",
    "        'target_num_ongoing_requests_per_replica': 10,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "default is 1 replica\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbde82-e3ab-486e-a0e3-1112d31e6fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29cae4aa-7c84-40fa-8f40-1cbd8bd64445",
   "metadata": {},
   "source": [
    "### Request batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f2939-95df-4328-bbf3-b497f5f7ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "@serve.deployment()\n",
    "class BatchAdder:\n",
    "    @serve.batch(max_batch_size=4)\n",
    "    async def handle_batch(self, numbers): # numbers is a list\n",
    "        input_array = np.array(numbers)\n",
    "        print(\"Our input array has shape:\", input_array.shape)\n",
    "        # Sleep for 200ms, this could be performing CPU intensive computation\n",
    "        # in real models\n",
    "        time.sleep(0.2)\n",
    "        output_array = input_array + 1\n",
    "        return output_array.astype(int).tolist()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        return await self.handle_batch(int(request.query_params[\"number\"]))\n",
    "    \n",
    "handle = serve.run(BatchAdder.bind(), name='batch_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a158e4-b81a-45f4-be40-a58494cb2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(i):\n",
    "    return requests.get(\"http://localhost:8000/adder?number={}\".format(i)).text\n",
    "\n",
    "make_request(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a38ec5-4b43-4e01-8ac0-07e996f1633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "executor = ThreadPoolExecutor()\n",
    "\n",
    "results = executor.map(make_request, range(0, 20, 2))\n",
    "\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d56dd1-46b7-45da-9a33-10deb32c20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete('batch_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc07e1-4574-419d-adb2-ca593d471963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
